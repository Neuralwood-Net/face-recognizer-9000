{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train_plot_and_save_224px_color.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m58",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neuralwood-Net/face-recognizer-9000/blob/main/notebooks/Train_plot_and_save_224px_color.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6k1V5shoVg"
      },
      "source": [
        "# Train networks on 224px color images\n",
        "### Notebook for training WoodNet and SqueezeNet on both the raw images and the images cropped to faces\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVI9ckKgrOFu"
      },
      "source": [
        "### Make sure the hardware is in order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1985DDBL4h",
        "outputId": "fce8d14a-1e08-4dad-edb6-e4d9a4a19bfa"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 18 09:25:09 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqcCktR3hrNG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rnSlQJGV9xh"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sys\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Placeholder to make it run until the real WoodNet is defined\n",
        "class WoodNet:\n",
        "    pass\n",
        "\n",
        "# Define an enumeration type for the different datasets\n",
        "# (easily extendable to more sets in the future)\n",
        "class Dataset:\n",
        "    RAW = 0\n",
        "    CROPPED = 1\n",
        "\n",
        "# Set what device to run the training on (preferrably GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdPDOarGkiF3"
      },
      "source": [
        "### Create configurations for the different datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-AT_GmckpFQ"
      },
      "source": [
        "# Define some configuration for the different datasets so the below code will work\n",
        "configuration = {\n",
        "    Dataset.RAW: {\n",
        "        \"blobname\": \"faces/balanced_sampled_224px_color_156240_images_70_15_15_split.zip\",\n",
        "        \"datadir\": \"sampled_dataset_balanced_244\",\n",
        "    },\n",
        "    Dataset.CROPPED: {\n",
        "        \"blobname\": \"faces/balanced_sampled_cropped_224px_color_70_15_15_split.tar.gz\",\n",
        "        \"datadir\": \"sampled_dataset_balanced_cropped_224\",\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKZU5hOmmi8e"
      },
      "source": [
        "### Decide what dataset to use for training of the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTlpcJuClI4w"
      },
      "source": [
        "# Available types are Dataset.RAW and Dataset.CROPPED\n",
        "# Choose one of them and continue\n",
        "# dataset = Dataset.CROPPED\n",
        "dataset = Dataset.CROPPED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gXbbcThtax"
      },
      "source": [
        "### Fetch and extract the data from the storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL7_2UEsx4IF"
      },
      "source": [
        "# Define paths for use later\n",
        "# (Kept separate from the heavy operations below in case they need to be rerun)\n",
        "BASE_PATH = \"/content\"\n",
        "\n",
        "BLOB_NAME = configuration[dataset][\"blobname\"]\n",
        "zipfilename = os.path.join(BASE_PATH, BLOB_NAME)\n",
        "extract_to_dir = os.path.join(BASE_PATH, *BLOB_NAME.split(os.path.sep)[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7t0dLiFcotJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "8d6564a4-45f7-4a52-da02-b4cee980ee68"
      },
      "source": [
        "# Make the required directories\n",
        "os.makedirs(os.path.join(BASE_PATH, \"faces\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"checkpoints\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"logs\"), exist_ok=True)\n",
        "\n",
        "# Fetch the data\n",
        "with open(zipfilename, \"wb\") as f:\n",
        "    storage.Client.create_anonymous_client().download_blob_to_file(f\"gs://tdt4173-datasets/{BLOB_NAME}\", f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36mdownload_blob_to_file\u001b[0;34m(self, blob_or_uri, file_obj, start, end)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mblob_or_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'download_to_file'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_file\u001b[0;34m(self, file_obj, client, start, end)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_download\u001b[0;34m(self, transport, file_obj, download_url, headers, start, end)\u001b[0m\n\u001b[1;32m    574\u001b[0m             )\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/resumable_media/requests/download.py\u001b[0m in \u001b[0;36mconsume\u001b[0;34m(self, transport)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/resumable_media/_download.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    170\u001b[0m         _helpers.require_status_code(\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ACCEPTABLE_STATUS_CODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/resumable_media/_helpers.py\u001b[0m in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;34mu\"Expected one of\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0mstatus_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ca304317294e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fetch the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_anonymous_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_blob_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"gs://tdt4173-datasets/{BLOB_NAME}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36mdownload_blob_to_file\u001b[0;34m(self, blob_or_uri, file_obj, start, end)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mblob_or_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mblob_or_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     def list_blobs(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_file\u001b[0;34m(self, file_obj, client, start, end)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0m_raise_from_invalid_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_raise_from_invalid_response\u001b[0;34m(error)\u001b[0m\n\u001b[1;32m   2036\u001b[0m     )\n\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2038\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 GET https://www.googleapis.com/download/storage/v1/b/tdt4173-datasets/o/faces%2Fbalanced_sampled_cropped_224px_color_70_15_15_split.tar.gz?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3lwkKMrOF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "818cdb6f-e48f-422a-9eee-dba4af6250eb"
      },
      "source": [
        "# Extract the data\n",
        "if dataset.CROPPED:\n",
        "    with tarfile.open(zipfilename, \"r:gz\") as f:\n",
        "        f.extractall(extract_to_dir)\n",
        "elif dataset.RAW:\n",
        "    with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_dir)\n",
        "else:\n",
        "    raise Exception(\"Invalid dataset chosen\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-af15f77930e2>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNORqJDwrOGC"
      },
      "source": [
        "### Load the data into wrapper classes and apply transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgCf231_Lsw"
      },
      "source": [
        "# Custom transform to reorder the color channels to the correct order\n",
        "class BGR2RGB:\n",
        "    def __call__(self, im):\n",
        "        b, g, r = im.split()\n",
        "        return Image.merge(\"RGB\", (r, g, b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSmppOms1mel"
      },
      "source": [
        "# Set how many images to process at once\n",
        "# Can be changed to accomodate lower memory devices\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "trans = [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "]\n",
        "\n",
        "# Color channel transform only applies to cropped dataset\n",
        "if dataset.CROPPED:\n",
        "    trans = [BGR2RGB()] + trans\n",
        "\n",
        "data_transforms = transforms.Compose(trans)\n",
        "\n",
        "data_dir = os.path.join(extract_to_dir, configuration[dataset])\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms)\n",
        "                  for x in ['train', 'val', 'test']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)\n",
        "print(image_datasets['val'].classes)\n",
        "print(dataset_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrnDUfOjrOGF"
      },
      "source": [
        "### Create a helper function to aid in image plotting and show a random sample of the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfMHbGt5YbQa"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['val']))\n",
        "\n",
        "inputs, classes = inputs[:8], classes[:8]\n",
        "\n",
        "print(inputs.shape)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVzf0lAul5y"
      },
      "source": [
        "### Create a function for training and validation\n",
        "The following function trains the supplied model with the loss criterion and optimizer supplied, for the specified number of epochs. During training it logs the loss and accuracy for both training and validation. Whenever a better model is found on the validation set, the function saves the model parameters to a file for use for inference later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3jtTxxvyxT4"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    modelname = f\"{type(model).__name__}-{since}\"\n",
        "    print(f\"Training model: `{type(model).__name__}`\")\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    num_img = {\n",
        "        \"train\": 0,\n",
        "        \"val\": 0,\n",
        "    }\n",
        "    \n",
        "    datapoints_per_epoch = 100\n",
        "\n",
        "    imgs_per_datapoint = {\n",
        "        \"train\": int(float(dataset_sizes[\"train\"] / datapoints_per_epoch)),\n",
        "        \"val\": int(float(dataset_sizes[\"val\"] / datapoints_per_epoch)),\n",
        "    }\n",
        "\n",
        "    print(\"Images per phase:\", imgs_per_datapoint[\"train\"], imgs_per_datapoint[\"val\"])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 10)\n",
        "        \n",
        "        with open(os.path.join(BASE_PATH, f\"logs/{modelname}.csv\"), \"a\") as f:\n",
        "\n",
        "            # For each epoch we want to both train and evaluate in that order\n",
        "            for phase in [\"train\", \"val\"]:\n",
        "                if phase == \"train\":\n",
        "                    # Makes the network ready for training, i.e. the parameters can be tuned\n",
        "                    # and possible Dropouts are activated\n",
        "                    model.train()\n",
        "                else:\n",
        "                    # Makes the network ready for inference, i.e. it is not tunable and will\n",
        "                    # turn off regularization that might interfere with training\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                plot_loss = 0\n",
        "                plot_corrects = 0\n",
        "\n",
        "                plot_points = 0\n",
        "\n",
        "                # Iterate over training or validation data\n",
        "                for inputs, labels in tqdm(dataloaders[phase], desc=f\"Epoch: {epoch} ({phase})\", file=sys.stdout):\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # Reset the gradients before calculating new ones\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    \n",
        "                    # Ask PyTorch to generate computation graph only if in training mode\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        \n",
        "                        # Only perform update steps if we're training\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "\n",
        "                    # Save values for statistics and logging\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    plot_loss += loss.item() * inputs.size(0)\n",
        "                    plot_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    num_img[phase] += BATCH_SIZE\n",
        "                    \n",
        "                    if (num_img[phase] % imgs_per_datapoint[phase]) < (BATCH_SIZE + 1):\n",
        "                        f.write(f\"{time.time()},{epoch},{phase},\\\n",
        "                        {num_img[phase]},{plot_loss / float(imgs_per_datapoint[phase])},\\\n",
        "                        {plot_corrects / float(imgs_per_datapoint[phase])}\\n\")\n",
        "                        \n",
        "                        plot_loss = 0\n",
        "                        plot_corrects = 0\n",
        "                        plot_points += 1\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "                print(f\"Points plotted: {plot_points}\")\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == \"val\" and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    # This saves the data that can be loaded later for inference\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"loss\": epoch_loss,\n",
        "                            \"acc\": epoch_acc,\n",
        "                            \"epoch\": epoch,\n",
        "                            \"parameters\": best_model_wts,\n",
        "                        },\n",
        "                        os.path.join(BASE_PATH, f\"checkpoints/{modelname}.data\"),\n",
        "                    )\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWDUGOMqh2jh"
      },
      "source": [
        "### Prepare the home-made CNN – WoodNet\n",
        "Below are two networks. The first is made by the authors, and is made to be trained from scratch on the training data. The other is fully trained on ImageNet (1000 classes) and fine-tuned on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2JPv3bfhm4_"
      },
      "source": [
        "class WoodNet(nn.Module):\n",
        "    # After 5 pooling layers the 224x224 images are reduced to 7x7\n",
        "    # while 3 channels have become 64.\n",
        "    size_after_conv = 7 * 7 * 64\n",
        "    def __init__(self):\n",
        "        super(WoodNet, self).__init__()\n",
        "        self.features = nn.Sequential(   \n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(self.size_after_conv, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, len(class_names)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.size_after_conv)\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the net and put it on currect device (preferrably the GPU)\n",
        "woodnet = WoodNet().to(device)\n",
        "woodnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3ms2adrOGO"
      },
      "source": [
        "### Prepare the pretrained CNN – SqueezeNet\n",
        "Below is the code for loading in the pretrained SqueezeNet. After it is loaded, the last classification layer is replaced with a one with the correct amount of output classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TtQyIlYrOGO"
      },
      "source": [
        "# Load pretrained SqueezeNet\n",
        "squeezenet = models.squeezenet1_1(pretrained=True, progress=True)\n",
        "# Replace the last layer with one with the correct number of channels\n",
        "num_ftr = squeezenet.classifier[1].in_channels\n",
        "squeezenet.classifier[1] = nn.Conv2d(num_ftr, len(class_names), 1, 1)\n",
        "# Create the net and put it on currect device (preferrably the GPU)\n",
        "squeezenet = squeezenet.to(device)\n",
        "squeezenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursGj99UCtt6"
      },
      "source": [
        "### Prepare a bad net – BadNet \n",
        "Below is a simple model which will be used for performance comparison with WoodNet and SqueezeNet. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yutnVu9Ho6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2599243e-d14d-4c4c-8187-e9f563897b41"
      },
      "source": [
        "class BadNet(nn.Module):\n",
        "    size_after_conv = 7 * 7 * 64\n",
        "    def __init__(self):\n",
        "        super(BadNet, self).__init__()\n",
        "        \n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(224*224*3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128, len(class_names)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 224*224*3)\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "badnet = BadNet().to(device)\n",
        "badnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BadNet(\n",
              "  (classify): Sequential(\n",
              "    (0): Linear(in_features=150528, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN9_8Ep4rOGQ"
      },
      "source": [
        "### Train the network\n",
        "Below is code that instantiates the loss function and optimization method and starts the training.\n",
        "To train every parameter in SqueezeNet, set `train_full_network = True`, and to `False` if only the last layer is to be trained. Set the variable network to the network that you want to train. Choices are `woodnet` and `squeezenet`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epKrvd6IzTeC"
      },
      "source": [
        "# network = squeezenet\n",
        "# network = badnet\n",
        "network = woodnet\n",
        "train_full_network = False\n",
        "\n",
        "if train_full_network or isinstance(network, (WoodNet, BadNet)):\n",
        "    print(\"Training full network\")\n",
        "    parameters = network.parameters()\n",
        "else:\n",
        "    print(\"Training only last layer of SqueezeNet\")\n",
        "    parameters = network.classifier[1].parameters()\n",
        "\n",
        "optimizer = torch.optim.SGD(parameters, lr=0.001, momentum=0.9)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(network, loss_function, optimizer, num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5F-eC4VSne"
      },
      "source": [
        "### Upload model weights and training logs to storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suEXstCJQzsv"
      },
      "source": [
        "# Upload checkpoints to storage\n",
        "client = storage.Client.from_service_account_json(\"/content/drive/My Drive/## Project/TDT4173 Deep Learning Project-91d3b469375c.json\")\n",
        "bucket = client.get_bucket(\"tdt4173-datasets\")\n",
        "\n",
        "blob = bucket.blob(\"checkpoints/SqueezeNet-1605361529.9021263_cropped.data\")\n",
        "filename = \"/content/checkpoints/SqueezeNet-1605361529.9021263.data\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"checkpoints/WoodNet-1605365270.1111202_cropped.data\")\n",
        "filename = \"/content/checkpoints/WoodNet-1605365270.1111202.data\"\n",
        "blob.upload_from_filename(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pexLiMsU-A2"
      },
      "source": [
        "# Upload logs to storage\n",
        "blob = bucket.blob(\"logs/SqueezeNet-1605290215.097698.csv\")\n",
        "filename = \"/content/logs/SqueezeNet-1605290215.097698.csv\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"logs/SqueezeNet-1605290736.1277423.csv\")\n",
        "filename = \"/content/logs/SqueezeNet-1605290736.1277423.csv\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"logs/WoodNet-1605294933.5362356.csv\")\n",
        "filename = \"/content/logs/WoodNet-1605294933.5362356.csv\"\n",
        "blob.upload_from_filename(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz-mn3TVrOGV"
      },
      "source": [
        "### Visualize the model performance for some images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mhJ5gmLy8CG"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WccAEZ9KrOGY"
      },
      "source": [
        "visualize_model(squeezenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wrdarsfoMY5"
      },
      "source": [
        "### Test the trained network with example images scraped from facebook etc. Can be used by uploading images and changing the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMJ4a5lX55e7"
      },
      "source": [
        "import cv2\n",
        "inputs = [\n",
        "    cv2.imread(\"/content/lars_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_3.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/ingvar_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/dwayne_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/kjartan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/faces/sampled_dataset_balanced_244/test/Kjartan/kjartan_video_5_9_augmentation_8.jpg\", cv2.IMREAD_COLOR),\n",
        "]\n",
        "\n",
        "for i, inp in enumerate(inputs):\n",
        "    inputs[i] = cv2.cvtColor(cv2.resize(inp, (244, 244)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def get_prediction_image(img, true_lab=None, plot=False):\n",
        "    assert not plot or (plot and true_lab)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    inp = cv2.resize(img, (224, 224)) / 255.0\n",
        "    inp = inp / std - mean\n",
        "    inp = inp.transpose((2, 0, 1))\n",
        "\n",
        "    imgt = torch.Tensor(inp).unsqueeze(0).to(device)\n",
        "\n",
        "    out = squeezenet(imgt)\n",
        "\n",
        "    probabilities = F.softmax(out, dim=1)\n",
        "\n",
        "    prob, class_idx = torch.max(probabilities, dim=1)\n",
        "    pred = class_names[class_idx]\n",
        "\n",
        "    if plot:\n",
        "        plt.imshow(img)\n",
        "        plt.text(5, 17,   f\"Actual   : {true_lab}\", color=\"white\", fontsize=14)\n",
        "        plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "\n",
        "    return pred, round(prob.item() * 100, 2), probabilities\n",
        "\n",
        "get_prediction_image(inputs[2], \"Morgan\", plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2c9a13X6ka0"
      },
      "source": [
        "plt.imshow(np.concatenate((inputs[1], inputs[2]), axis=1))\n",
        "(pred, prob), actual = get_prediction_image(inputs[1]), \"Morgan\"\n",
        "plt.text(5, 17,   f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "\n",
        "(pred, prob), actual = get_prediction_image(inputs[2]), \"Morgan\"\n",
        "plt.text(249, 17, f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 34, f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "plt.savefig(\"morgan_crop_plot.png\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHq7ps3PoiSu"
      },
      "source": [
        "### Code for calculating the accuracy, loss, precision, and recall for all the networks and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ABu6yANpSc"
      },
      "source": [
        "# Load saved data from a training run\n",
        "# and initialize model weights to the saved weights\n",
        "# save_data = \"/content/SqueezeNet-1605290736.1277423.data\"\n",
        "save_data = \"/content/WoodNet-1605365270.1111202_cropped.data\"\n",
        "network.load_state_dict(torch.load(save_data)[\"parameters\"])\n",
        "network = network.to(device).eval()\n",
        "\n",
        "running_loss = 0\n",
        "running_corrects = 0\n",
        "\n",
        "# Define what classes have access\n",
        "access = set([0, 1, 2])\n",
        "\n",
        "# Counters for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for inputs, labels in tqdm(dataloaders[\"test\"]):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Save values for statistics and logging\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        for pred, lab in zip(preds, labels):\n",
        "            pred, lab = pred.item(), lab.item()\n",
        "            if lab in access and pred in access:\n",
        "                tp += 1\n",
        "            elif lab in access and pred not in access:\n",
        "                fn += 1\n",
        "            elif lab not in access and pred in access:\n",
        "                fp += 1\n",
        "\n",
        "\n",
        "# Loss\n",
        "print(running_loss / dataset_sizes[\"test\"])\n",
        "\n",
        "# Accuracy\n",
        "print(running_corrects.double() / dataset_sizes[\"test\"])\n",
        "\n",
        "# Precision\n",
        "print(tp / (tp + fp))\n",
        "\n",
        "# Recall\n",
        "print(tp / (tp + fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um68jE4TWOen"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0b5Qu2sp01"
      },
      "source": [
        "### Create confusion matrix for the WoodNet, SqueezeNet, and BadNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbHT8qJtDh8_"
      },
      "source": [
        "Below is a function which returns all true lables and a models predicted values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmqJXZNTyqCU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def get_all_preds_labels(model, loader):\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "\n",
        "    for inputs, labels in tqdm(loader):\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        preds = model(inputs)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "        all_labels = torch.cat(\n",
        "            (all_labels, labels)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds, all_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXs8fF-1Cs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb742ef7-e6a3-4783-b2dd-dc2f6aaa2f5d"
      },
      "source": [
        "test_preds, test_labels = get_all_preds_labels(squeezenet,dataloaders[\"test\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1465/1465 [00:55<00:00, 26.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhwawXE1s2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7280a7ce-d95d-4147-c079-934bb61fbdbb"
      },
      "source": [
        " #[actual,predicted]\n",
        " stacked = torch.stack(\n",
        "    (\n",
        "        test_labels\n",
        "        ,test_preds.argmax(dim=1)\n",
        "    )\n",
        "    ,dim=1\n",
        ")\n",
        "stacked.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([23436, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YgRCi3sDvQm"
      },
      "source": [
        "This is a convolutional matrix in tensor form, not used for the visualisation, but we provide it as it can be useful in some cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XToO6Pq1KJ0"
      },
      "source": [
        "def cm_tensor(stacked):\n",
        "  cm_tensor = torch.zeros(4,4, dtype=torch.int64)\n",
        "  for pair in stacked:\n",
        "    al,pl = pair.tolist()\n",
        "    cm_tensor[int(al),int(pl)] = cm_tensor[int(al),int(pl)] + 1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdpyIDKh3HCW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEGfOdRhD3r5"
      },
      "source": [
        "The plotting is done on the cpu. As such we return our calculated test predictions and acutal labels to be calculated on the cpu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS7LsSON5E8w"
      },
      "source": [
        "\n",
        "test_labels = test_labels.cpu()\n",
        "test_preds = test_preds.cpu()\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds.argmax(dim=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS7vIhH0EDxA"
      },
      "source": [
        "The below plotter is based on the code from DeepLizard: https://deeplizard.com/learn/video/0LhiS6yu2qQ?fbclid=IwAR1Zb3LSBe4nhuxa6OhwpW4-rXwg7LhMIeG0C0iCWMrYLH2Bkhfh-z5IaL0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrZWza7pBfYY"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, filename, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "        pass\n",
        "\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "# plot_confusion_matrix(cm,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_zuCWeJDAm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f804e7f-1211-40a8-905f-9681326979a3"
      },
      "source": [
        "#test_preds, test_labels = get_all_preds_labels(model,dataloader_and_type)\n",
        "\n",
        "\n",
        "def cnn_constructor(preds,labels,filename,title): \n",
        "    stacked = torch.stack(\n",
        "      (\n",
        "          labels\n",
        "          ,preds.argmax(dim=1)\n",
        "      )\n",
        "      ,dim=1\n",
        "    )\n",
        "    labels = labels.cpu()\n",
        "    preds = preds.cpu()\n",
        "    cm = confusion_matrix(labels, preds.argmax(dim=1))\n",
        "    plot_confusion_matrix(cm,class_names,filename,title=title)\n",
        "#SqueezeNet\n",
        "#test_preds_s,test_labels_s=get_all_preds_labels(squeezenet,dataloaders[\"test\"])\n",
        "#cnn_constructor(test_preds_s,test_labels_s,\"/content/squeezenet_cm.png\",title=\"SqueezeNet Confusion Matrix\");\n",
        "#WoodNet\n",
        "test_preds_w,test_labels_w = get_all_preds_labels(woodnet,dataloaders[\"test\"])\n",
        "cnn_constructor(test_preds_w,test_labels_w,\"/content/woodnet_cm.png\",title=\"WoodNet Confusion Matrix\");\n",
        "\n",
        "# test_preds,test_labels = get_all_preds_labels(badnet,dataloaders[\"test\"])\n",
        "#cnn_constructor(test_preds,test_labels,\"/content/badnet_cm.png\",title=\"BadNet Confusion Matrix\");\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1465 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/1465 [00:00<06:52,  3.55it/s]\u001b[A\n",
            "  0%|          | 2/1465 [00:00<06:08,  3.97it/s]\u001b[A\n",
            "  0%|          | 6/1465 [00:00<04:34,  5.31it/s]\u001b[A\n",
            "  1%|          | 10/1465 [00:00<03:29,  6.95it/s]\u001b[A\n",
            "  1%|          | 14/1465 [00:00<02:43,  8.88it/s]\u001b[A\n",
            "  1%|          | 18/1465 [00:01<02:11, 10.97it/s]\u001b[A\n",
            "  2%|▏         | 22/1465 [00:01<01:50, 13.06it/s]\u001b[A\n",
            "  2%|▏         | 26/1465 [00:01<01:36, 14.91it/s]\u001b[A\n",
            "  2%|▏         | 30/1465 [00:01<01:19, 18.02it/s]\u001b[A\n",
            "  2%|▏         | 34/1465 [00:01<01:12, 19.82it/s]\u001b[A\n",
            "  3%|▎         | 38/1465 [00:01<01:05, 21.91it/s]\u001b[A\n",
            "  3%|▎         | 42/1465 [00:02<01:01, 22.97it/s]\u001b[A\n",
            "  3%|▎         | 46/1465 [00:02<01:01, 23.12it/s]\u001b[A\n",
            "  3%|▎         | 50/1465 [00:02<01:00, 23.51it/s]\u001b[A\n",
            "  4%|▎         | 54/1465 [00:02<00:57, 24.41it/s]\u001b[A\n",
            "  4%|▍         | 58/1465 [00:02<00:55, 25.41it/s]\u001b[A\n",
            "  4%|▍         | 61/1465 [00:02<00:52, 26.59it/s]\u001b[A\n",
            "  4%|▍         | 64/1465 [00:02<00:52, 26.46it/s]\u001b[A\n",
            "  5%|▍         | 67/1465 [00:02<00:51, 26.93it/s]\u001b[A\n",
            "  5%|▍         | 70/1465 [00:03<00:55, 25.04it/s]\u001b[A\n",
            "  5%|▍         | 73/1465 [00:03<00:53, 25.82it/s]\u001b[A\n",
            "  5%|▌         | 77/1465 [00:03<00:58, 23.74it/s]\u001b[A\n",
            "  6%|▌         | 81/1465 [00:03<00:59, 23.15it/s]\u001b[A\n",
            "  6%|▌         | 85/1465 [00:03<00:55, 25.03it/s]\u001b[A\n",
            "  6%|▌         | 89/1465 [00:03<00:54, 25.02it/s]\u001b[A\n",
            "  6%|▋         | 93/1465 [00:04<00:55, 24.86it/s]\u001b[A\n",
            "  7%|▋         | 97/1465 [00:04<00:56, 24.22it/s]\u001b[A\n",
            "  7%|▋         | 101/1465 [00:04<01:00, 22.64it/s]\u001b[A\n",
            "  7%|▋         | 105/1465 [00:04<00:57, 23.55it/s]\u001b[A\n",
            "  7%|▋         | 109/1465 [00:04<00:56, 23.80it/s]\u001b[A\n",
            "  8%|▊         | 113/1465 [00:04<00:52, 25.64it/s]\u001b[A\n",
            "  8%|▊         | 117/1465 [00:05<00:53, 25.25it/s]\u001b[A\n",
            "  8%|▊         | 121/1465 [00:05<00:48, 27.47it/s]\u001b[A\n",
            "  9%|▊         | 125/1465 [00:05<00:44, 29.88it/s]\u001b[A\n",
            "  9%|▉         | 129/1465 [00:05<00:46, 28.81it/s]\u001b[A\n",
            "  9%|▉         | 132/1465 [00:05<00:50, 26.20it/s]\u001b[A\n",
            "  9%|▉         | 135/1465 [00:05<00:53, 24.63it/s]\u001b[A\n",
            "  9%|▉         | 138/1465 [00:05<00:57, 23.07it/s]\u001b[A\n",
            " 10%|▉         | 142/1465 [00:05<00:53, 24.50it/s]\u001b[A\n",
            " 10%|▉         | 146/1465 [00:06<00:52, 25.08it/s]\u001b[A\n",
            " 10%|█         | 149/1465 [00:06<00:50, 26.29it/s]\u001b[A\n",
            " 10%|█         | 152/1465 [00:06<00:48, 26.97it/s]\u001b[A\n",
            " 11%|█         | 155/1465 [00:06<00:51, 25.37it/s]\u001b[A\n",
            " 11%|█         | 158/1465 [00:06<00:56, 23.00it/s]\u001b[A\n",
            " 11%|█         | 162/1465 [00:06<00:55, 23.56it/s]\u001b[A\n",
            " 11%|█▏        | 166/1465 [00:06<00:55, 23.54it/s]\u001b[A\n",
            " 12%|█▏        | 170/1465 [00:07<00:55, 23.42it/s]\u001b[A\n",
            " 12%|█▏        | 174/1465 [00:07<00:54, 23.58it/s]\u001b[A\n",
            " 12%|█▏        | 178/1465 [00:07<00:54, 23.50it/s]\u001b[A\n",
            " 12%|█▏        | 182/1465 [00:07<00:56, 22.83it/s]\u001b[A\n",
            " 13%|█▎        | 186/1465 [00:07<00:53, 23.93it/s]\u001b[A\n",
            " 13%|█▎        | 190/1465 [00:07<00:53, 23.78it/s]\u001b[A\n",
            " 13%|█▎        | 194/1465 [00:08<00:51, 24.69it/s]\u001b[A\n",
            " 14%|█▎        | 198/1465 [00:08<00:48, 25.97it/s]\u001b[A\n",
            " 14%|█▍        | 202/1465 [00:08<00:48, 26.31it/s]\u001b[A\n",
            " 14%|█▍        | 206/1465 [00:08<00:45, 27.78it/s]\u001b[A\n",
            " 14%|█▍        | 210/1465 [00:08<00:44, 28.35it/s]\u001b[A\n",
            " 15%|█▍        | 213/1465 [00:08<00:44, 27.86it/s]\u001b[A\n",
            " 15%|█▍        | 216/1465 [00:08<00:52, 23.99it/s]\u001b[A\n",
            " 15%|█▍        | 219/1465 [00:09<00:48, 25.50it/s]\u001b[A\n",
            " 15%|█▌        | 222/1465 [00:09<00:49, 24.90it/s]\u001b[A\n",
            " 15%|█▌        | 225/1465 [00:09<00:51, 24.26it/s]\u001b[A\n",
            " 16%|█▌        | 228/1465 [00:09<00:54, 22.49it/s]\u001b[A\n",
            " 16%|█▌        | 232/1465 [00:09<00:56, 21.99it/s]\u001b[A\n",
            " 16%|█▌        | 236/1465 [00:09<00:48, 25.32it/s]\u001b[A\n",
            " 16%|█▋        | 240/1465 [00:09<00:49, 24.62it/s]\u001b[A\n",
            " 17%|█▋        | 244/1465 [00:10<00:47, 25.84it/s]\u001b[A\n",
            " 17%|█▋        | 248/1465 [00:10<00:44, 27.11it/s]\u001b[A\n",
            " 17%|█▋        | 252/1465 [00:10<00:44, 27.18it/s]\u001b[A\n",
            " 17%|█▋        | 255/1465 [00:10<00:43, 27.90it/s]\u001b[A\n",
            " 18%|█▊        | 258/1465 [00:10<00:43, 27.82it/s]\u001b[A\n",
            " 18%|█▊        | 261/1465 [00:10<00:49, 24.28it/s]\u001b[A\n",
            " 18%|█▊        | 264/1465 [00:10<00:52, 22.87it/s]\u001b[A\n",
            " 18%|█▊        | 267/1465 [00:11<00:52, 23.00it/s]\u001b[A\n",
            " 18%|█▊        | 271/1465 [00:11<00:52, 22.80it/s]\u001b[A\n",
            " 19%|█▉        | 275/1465 [00:11<00:51, 23.29it/s]\u001b[A\n",
            " 19%|█▉        | 279/1465 [00:11<00:47, 24.86it/s]\u001b[A\n",
            " 19%|█▉        | 283/1465 [00:11<00:48, 24.44it/s]\u001b[A\n",
            " 20%|█▉        | 287/1465 [00:11<00:51, 22.91it/s]\u001b[A\n",
            " 20%|█▉        | 291/1465 [00:11<00:47, 24.73it/s]\u001b[A\n",
            " 20%|██        | 295/1465 [00:12<00:45, 25.96it/s]\u001b[A\n",
            " 20%|██        | 299/1465 [00:12<00:41, 27.88it/s]\u001b[A\n",
            " 21%|██        | 302/1465 [00:12<00:43, 26.84it/s]\u001b[A\n",
            " 21%|██        | 305/1465 [00:12<00:43, 26.75it/s]\u001b[A\n",
            " 21%|██        | 308/1465 [00:12<00:46, 24.95it/s]\u001b[A\n",
            " 21%|██        | 311/1465 [00:12<00:49, 23.20it/s]\u001b[A\n",
            " 22%|██▏       | 315/1465 [00:12<00:53, 21.54it/s]\u001b[A\n",
            " 22%|██▏       | 320/1465 [00:13<00:48, 23.53it/s]\u001b[A\n",
            " 22%|██▏       | 324/1465 [00:13<00:46, 24.74it/s]\u001b[A\n",
            " 22%|██▏       | 328/1465 [00:13<00:42, 26.54it/s]\u001b[A\n",
            " 23%|██▎       | 331/1465 [00:13<00:48, 23.51it/s]\u001b[A\n",
            " 23%|██▎       | 334/1465 [00:13<00:45, 24.80it/s]\u001b[A\n",
            " 23%|██▎       | 337/1465 [00:13<00:46, 24.09it/s]\u001b[A\n",
            " 23%|██▎       | 340/1465 [00:13<00:50, 22.30it/s]\u001b[A\n",
            " 23%|██▎       | 344/1465 [00:14<00:44, 25.40it/s]\u001b[A\n",
            " 24%|██▎       | 347/1465 [00:14<00:46, 23.90it/s]\u001b[A\n",
            " 24%|██▍       | 351/1465 [00:14<00:44, 24.86it/s]\u001b[A\n",
            " 24%|██▍       | 355/1465 [00:14<00:45, 24.20it/s]\u001b[A\n",
            " 25%|██▍       | 359/1465 [00:14<00:42, 25.81it/s]\u001b[A\n",
            " 25%|██▍       | 363/1465 [00:14<00:43, 25.24it/s]\u001b[A\n",
            " 25%|██▌       | 367/1465 [00:14<00:41, 26.71it/s]\u001b[A\n",
            " 25%|██▌       | 370/1465 [00:15<00:40, 26.79it/s]\u001b[A\n",
            " 26%|██▌       | 374/1465 [00:15<00:41, 26.10it/s]\u001b[A\n",
            " 26%|██▌       | 377/1465 [00:15<00:40, 27.05it/s]\u001b[A\n",
            " 26%|██▌       | 380/1465 [00:15<00:40, 26.79it/s]\u001b[A\n",
            " 26%|██▌       | 383/1465 [00:15<00:45, 23.75it/s]\u001b[A\n",
            " 26%|██▋       | 386/1465 [00:15<00:49, 21.64it/s]\u001b[A\n",
            " 27%|██▋       | 390/1465 [00:15<00:47, 22.49it/s]\u001b[A\n",
            " 27%|██▋       | 394/1465 [00:16<00:46, 23.12it/s]\u001b[A\n",
            " 27%|██▋       | 398/1465 [00:16<00:45, 23.31it/s]\u001b[A\n",
            " 27%|██▋       | 402/1465 [00:16<00:40, 26.33it/s]\u001b[A\n",
            " 28%|██▊       | 405/1465 [00:16<00:40, 25.95it/s]\u001b[A\n",
            " 28%|██▊       | 408/1465 [00:16<00:41, 25.58it/s]\u001b[A\n",
            " 28%|██▊       | 411/1465 [00:16<00:41, 25.70it/s]\u001b[A\n",
            " 28%|██▊       | 414/1465 [00:16<00:41, 25.10it/s]\u001b[A\n",
            " 28%|██▊       | 417/1465 [00:16<00:40, 26.12it/s]\u001b[A\n",
            " 29%|██▊       | 420/1465 [00:17<00:44, 23.69it/s]\u001b[A\n",
            " 29%|██▉       | 424/1465 [00:17<00:43, 23.98it/s]\u001b[A\n",
            " 29%|██▉       | 428/1465 [00:17<00:40, 25.40it/s]\u001b[A\n",
            " 29%|██▉       | 432/1465 [00:17<00:39, 26.10it/s]\u001b[A\n",
            " 30%|██▉       | 435/1465 [00:17<00:39, 25.83it/s]\u001b[A\n",
            " 30%|██▉       | 438/1465 [00:17<00:43, 23.44it/s]\u001b[A\n",
            " 30%|███       | 442/1465 [00:17<00:43, 23.62it/s]\u001b[A\n",
            " 30%|███       | 446/1465 [00:18<00:41, 24.69it/s]\u001b[A\n",
            " 31%|███       | 450/1465 [00:18<00:38, 26.38it/s]\u001b[A\n",
            " 31%|███       | 454/1465 [00:18<00:40, 25.18it/s]\u001b[A\n",
            " 31%|███▏      | 458/1465 [00:18<00:41, 24.26it/s]\u001b[A\n",
            " 32%|███▏      | 462/1465 [00:18<00:43, 22.96it/s]\u001b[A\n",
            " 32%|███▏      | 466/1465 [00:18<00:41, 24.10it/s]\u001b[A\n",
            " 32%|███▏      | 470/1465 [00:19<00:39, 25.22it/s]\u001b[A\n",
            " 32%|███▏      | 474/1465 [00:19<00:40, 24.32it/s]\u001b[A\n",
            " 33%|███▎      | 478/1465 [00:19<00:39, 24.99it/s]\u001b[A\n",
            " 33%|███▎      | 482/1465 [00:19<00:35, 27.42it/s]\u001b[A\n",
            " 33%|███▎      | 485/1465 [00:19<00:38, 25.28it/s]\u001b[A\n",
            " 33%|███▎      | 488/1465 [00:19<00:43, 22.27it/s]\u001b[A\n",
            " 34%|███▎      | 492/1465 [00:19<00:38, 25.04it/s]\u001b[A\n",
            " 34%|███▍      | 495/1465 [00:20<00:41, 23.28it/s]\u001b[A\n",
            " 34%|███▍      | 499/1465 [00:20<00:41, 23.21it/s]\u001b[A\n",
            " 34%|███▍      | 503/1465 [00:20<00:42, 22.54it/s]\u001b[A\n",
            " 35%|███▍      | 507/1465 [00:20<00:38, 24.69it/s]\u001b[A\n",
            " 35%|███▍      | 511/1465 [00:20<00:39, 24.35it/s]\u001b[A\n",
            " 35%|███▌      | 515/1465 [00:20<00:39, 24.12it/s]\u001b[A\n",
            " 35%|███▌      | 519/1465 [00:21<00:39, 23.91it/s]\u001b[A\n",
            " 36%|███▌      | 523/1465 [00:21<00:37, 25.20it/s]\u001b[A\n",
            " 36%|███▌      | 527/1465 [00:21<00:35, 26.32it/s]\u001b[A\n",
            " 36%|███▌      | 531/1465 [00:21<00:35, 26.54it/s]\u001b[A\n",
            " 37%|███▋      | 535/1465 [00:21<00:32, 28.53it/s]\u001b[A\n",
            " 37%|███▋      | 539/1465 [00:21<00:34, 26.75it/s]\u001b[A\n",
            " 37%|███▋      | 543/1465 [00:21<00:34, 26.41it/s]\u001b[A\n",
            " 37%|███▋      | 546/1465 [00:22<00:34, 26.38it/s]\u001b[A\n",
            " 37%|███▋      | 549/1465 [00:22<00:35, 25.88it/s]\u001b[A\n",
            " 38%|███▊      | 552/1465 [00:22<00:38, 23.83it/s]\u001b[A\n",
            " 38%|███▊      | 556/1465 [00:22<00:36, 24.95it/s]\u001b[A\n",
            " 38%|███▊      | 559/1465 [00:22<00:34, 25.92it/s]\u001b[A\n",
            " 38%|███▊      | 562/1465 [00:22<00:38, 23.67it/s]\u001b[A\n",
            " 39%|███▊      | 565/1465 [00:22<00:39, 22.76it/s]\u001b[A\n",
            " 39%|███▉      | 569/1465 [00:23<00:40, 21.97it/s]\u001b[A\n",
            " 39%|███▉      | 573/1465 [00:23<00:37, 23.93it/s]\u001b[A\n",
            " 39%|███▉      | 577/1465 [00:23<00:35, 25.26it/s]\u001b[A\n",
            " 40%|███▉      | 581/1465 [00:23<00:34, 25.88it/s]\u001b[A\n",
            " 40%|███▉      | 585/1465 [00:23<00:35, 24.87it/s]\u001b[A\n",
            " 40%|████      | 589/1465 [00:23<00:35, 24.44it/s]\u001b[A\n",
            " 40%|████      | 593/1465 [00:24<00:36, 23.92it/s]\u001b[A\n",
            " 41%|████      | 597/1465 [00:24<00:36, 23.52it/s]\u001b[A\n",
            " 41%|████      | 601/1465 [00:24<00:34, 24.72it/s]\u001b[A\n",
            " 41%|████▏     | 605/1465 [00:24<00:32, 26.10it/s]\u001b[A\n",
            " 42%|████▏     | 609/1465 [00:24<00:33, 25.65it/s]\u001b[A\n",
            " 42%|████▏     | 613/1465 [00:24<00:36, 23.49it/s]\u001b[A\n",
            " 42%|████▏     | 617/1465 [00:25<00:36, 23.52it/s]\u001b[A\n",
            " 42%|████▏     | 621/1465 [00:25<00:32, 25.70it/s]\u001b[A\n",
            " 43%|████▎     | 625/1465 [00:25<00:34, 24.12it/s]\u001b[A\n",
            " 43%|████▎     | 629/1465 [00:25<00:34, 24.10it/s]\u001b[A\n",
            " 43%|████▎     | 633/1465 [00:25<00:35, 23.48it/s]\u001b[A\n",
            " 43%|████▎     | 637/1465 [00:25<00:33, 24.89it/s]\u001b[A\n",
            " 44%|████▍     | 641/1465 [00:25<00:32, 25.56it/s]\u001b[A\n",
            " 44%|████▍     | 645/1465 [00:26<00:31, 25.91it/s]\u001b[A\n",
            " 44%|████▍     | 649/1465 [00:26<00:31, 25.68it/s]\u001b[A\n",
            " 45%|████▍     | 653/1465 [00:26<00:32, 24.88it/s]\u001b[A\n",
            " 45%|████▍     | 656/1465 [00:26<00:31, 26.08it/s]\u001b[A\n",
            " 45%|████▍     | 659/1465 [00:26<00:32, 25.13it/s]\u001b[A\n",
            " 45%|████▌     | 662/1465 [00:26<00:34, 22.97it/s]\u001b[A\n",
            " 45%|████▌     | 666/1465 [00:27<00:33, 23.51it/s]\u001b[A\n",
            " 46%|████▌     | 670/1465 [00:27<00:32, 24.49it/s]\u001b[A\n",
            " 46%|████▌     | 674/1465 [00:27<00:29, 26.53it/s]\u001b[A\n",
            " 46%|████▋     | 678/1465 [00:27<00:33, 23.37it/s]\u001b[A\n",
            " 47%|████▋     | 682/1465 [00:27<00:30, 25.89it/s]\u001b[A\n",
            " 47%|████▋     | 686/1465 [00:27<00:30, 25.44it/s]\u001b[A\n",
            " 47%|████▋     | 690/1465 [00:27<00:29, 26.33it/s]\u001b[A\n",
            " 47%|████▋     | 693/1465 [00:28<00:29, 26.42it/s]\u001b[A\n",
            " 48%|████▊     | 697/1465 [00:28<00:30, 25.17it/s]\u001b[A\n",
            " 48%|████▊     | 701/1465 [00:28<00:30, 24.99it/s]\u001b[A\n",
            " 48%|████▊     | 705/1465 [00:28<00:30, 24.56it/s]\u001b[A\n",
            " 48%|████▊     | 709/1465 [00:28<00:30, 24.89it/s]\u001b[A\n",
            " 49%|████▊     | 713/1465 [00:28<00:31, 24.06it/s]\u001b[A\n",
            " 49%|████▉     | 717/1465 [00:29<00:30, 24.33it/s]\u001b[A\n",
            " 49%|████▉     | 721/1465 [00:29<00:29, 24.88it/s]\u001b[A\n",
            " 49%|████▉     | 725/1465 [00:29<00:29, 25.05it/s]\u001b[A\n",
            " 50%|████▉     | 729/1465 [00:29<00:29, 24.61it/s]\u001b[A\n",
            " 50%|█████     | 733/1465 [00:29<00:29, 24.64it/s]\u001b[A\n",
            " 50%|█████     | 737/1465 [00:29<00:29, 24.75it/s]\u001b[A\n",
            " 51%|█████     | 741/1465 [00:29<00:28, 25.62it/s]\u001b[A\n",
            " 51%|█████     | 745/1465 [00:30<00:27, 26.51it/s]\u001b[A\n",
            " 51%|█████     | 749/1465 [00:30<00:26, 27.23it/s]\u001b[A\n",
            " 51%|█████▏    | 752/1465 [00:30<00:27, 25.94it/s]\u001b[A\n",
            " 52%|█████▏    | 755/1465 [00:30<00:29, 23.74it/s]\u001b[A\n",
            " 52%|█████▏    | 759/1465 [00:30<00:27, 25.51it/s]\u001b[A\n",
            " 52%|█████▏    | 762/1465 [00:30<00:28, 25.05it/s]\u001b[A\n",
            " 52%|█████▏    | 765/1465 [00:30<00:28, 24.92it/s]\u001b[A\n",
            " 52%|█████▏    | 769/1465 [00:31<00:25, 26.81it/s]\u001b[A\n",
            " 53%|█████▎    | 772/1465 [00:31<00:28, 23.95it/s]\u001b[A\n",
            " 53%|█████▎    | 775/1465 [00:31<00:30, 22.65it/s]\u001b[A\n",
            " 53%|█████▎    | 778/1465 [00:31<00:31, 22.03it/s]\u001b[A\n",
            " 53%|█████▎    | 782/1465 [00:31<00:30, 22.09it/s]\u001b[A\n",
            " 54%|█████▎    | 786/1465 [00:31<00:29, 23.39it/s]\u001b[A\n",
            " 54%|█████▍    | 790/1465 [00:31<00:27, 24.79it/s]\u001b[A\n",
            " 54%|█████▍    | 793/1465 [00:32<00:29, 23.16it/s]\u001b[A\n",
            " 54%|█████▍    | 797/1465 [00:32<00:29, 22.88it/s]\u001b[A\n",
            " 55%|█████▍    | 801/1465 [00:32<00:28, 23.17it/s]\u001b[A\n",
            " 55%|█████▍    | 805/1465 [00:32<00:28, 23.32it/s]\u001b[A\n",
            " 55%|█████▌    | 809/1465 [00:32<00:27, 23.70it/s]\u001b[A\n",
            " 55%|█████▌    | 813/1465 [00:32<00:25, 25.73it/s]\u001b[A\n",
            " 56%|█████▌    | 817/1465 [00:33<00:25, 25.37it/s]\u001b[A\n",
            " 56%|█████▌    | 821/1465 [00:33<00:27, 23.52it/s]\u001b[A\n",
            " 56%|█████▋    | 825/1465 [00:33<00:24, 25.84it/s]\u001b[A\n",
            " 57%|█████▋    | 829/1465 [00:33<00:23, 27.03it/s]\u001b[A\n",
            " 57%|█████▋    | 832/1465 [00:33<00:23, 26.58it/s]\u001b[A\n",
            " 57%|█████▋    | 835/1465 [00:33<00:25, 24.65it/s]\u001b[A\n",
            " 57%|█████▋    | 839/1465 [00:33<00:23, 26.26it/s]\u001b[A\n",
            " 57%|█████▋    | 842/1465 [00:34<00:25, 24.76it/s]\u001b[A\n",
            " 58%|█████▊    | 845/1465 [00:34<00:25, 24.52it/s]\u001b[A\n",
            " 58%|█████▊    | 848/1465 [00:34<00:24, 25.63it/s]\u001b[A\n",
            " 58%|█████▊    | 851/1465 [00:34<00:24, 25.56it/s]\u001b[A\n",
            " 58%|█████▊    | 854/1465 [00:34<00:25, 23.95it/s]\u001b[A\n",
            " 58%|█████▊    | 857/1465 [00:34<00:27, 22.02it/s]\u001b[A\n",
            " 59%|█████▉    | 861/1465 [00:34<00:26, 22.91it/s]\u001b[A\n",
            " 59%|█████▉    | 865/1465 [00:35<00:26, 22.85it/s]\u001b[A\n",
            " 59%|█████▉    | 869/1465 [00:35<00:25, 23.73it/s]\u001b[A\n",
            " 60%|█████▉    | 873/1465 [00:35<00:25, 23.30it/s]\u001b[A\n",
            " 60%|█████▉    | 877/1465 [00:35<00:25, 23.45it/s]\u001b[A\n",
            " 60%|██████    | 881/1465 [00:35<00:24, 23.69it/s]\u001b[A\n",
            " 60%|██████    | 885/1465 [00:35<00:22, 25.86it/s]\u001b[A\n",
            " 61%|██████    | 889/1465 [00:35<00:22, 26.06it/s]\u001b[A\n",
            " 61%|██████    | 893/1465 [00:36<00:21, 26.39it/s]\u001b[A\n",
            " 61%|██████    | 897/1465 [00:36<00:21, 25.95it/s]\u001b[A\n",
            " 62%|██████▏   | 901/1465 [00:36<00:23, 24.32it/s]\u001b[A\n",
            " 62%|██████▏   | 905/1465 [00:36<00:22, 24.93it/s]\u001b[A\n",
            " 62%|██████▏   | 909/1465 [00:36<00:21, 25.46it/s]\u001b[A\n",
            " 62%|██████▏   | 913/1465 [00:36<00:22, 25.02it/s]\u001b[A\n",
            " 63%|██████▎   | 917/1465 [00:37<00:21, 25.03it/s]\u001b[A\n",
            " 63%|██████▎   | 921/1465 [00:37<00:22, 24.61it/s]\u001b[A\n",
            " 63%|██████▎   | 925/1465 [00:37<00:21, 24.77it/s]\u001b[A\n",
            " 63%|██████▎   | 929/1465 [00:37<00:21, 24.91it/s]\u001b[A\n",
            " 64%|██████▎   | 933/1465 [00:37<00:21, 24.62it/s]\u001b[A\n",
            " 64%|██████▍   | 937/1465 [00:37<00:22, 23.25it/s]\u001b[A\n",
            " 64%|██████▍   | 941/1465 [00:38<00:20, 25.44it/s]\u001b[A\n",
            " 65%|██████▍   | 945/1465 [00:38<00:20, 25.09it/s]\u001b[A\n",
            " 65%|██████▍   | 949/1465 [00:38<00:20, 24.62it/s]\u001b[A\n",
            " 65%|██████▌   | 953/1465 [00:38<00:21, 23.97it/s]\u001b[A\n",
            " 65%|██████▌   | 957/1465 [00:38<00:20, 24.45it/s]\u001b[A\n",
            " 66%|██████▌   | 961/1465 [00:38<00:20, 24.99it/s]\u001b[A\n",
            " 66%|██████▌   | 965/1465 [00:39<00:20, 24.27it/s]\u001b[A\n",
            " 66%|██████▌   | 969/1465 [00:39<00:19, 25.70it/s]\u001b[A\n",
            " 66%|██████▋   | 973/1465 [00:39<00:19, 24.63it/s]\u001b[A\n",
            " 67%|██████▋   | 977/1465 [00:39<00:20, 23.94it/s]\u001b[A\n",
            " 67%|██████▋   | 981/1465 [00:39<00:19, 25.04it/s]\u001b[A\n",
            " 67%|██████▋   | 985/1465 [00:39<00:18, 25.69it/s]\u001b[A\n",
            " 68%|██████▊   | 989/1465 [00:39<00:18, 25.47it/s]\u001b[A\n",
            " 68%|██████▊   | 993/1465 [00:40<00:18, 26.06it/s]\u001b[A\n",
            " 68%|██████▊   | 997/1465 [00:40<00:19, 24.57it/s]\u001b[A\n",
            " 68%|██████▊   | 1001/1465 [00:40<00:18, 25.32it/s]\u001b[A\n",
            " 69%|██████▊   | 1005/1465 [00:40<00:18, 25.17it/s]\u001b[A\n",
            " 69%|██████▉   | 1009/1465 [00:40<00:18, 24.73it/s]\u001b[A\n",
            " 69%|██████▉   | 1013/1465 [00:40<00:19, 23.66it/s]\u001b[A\n",
            " 69%|██████▉   | 1017/1465 [00:41<00:18, 24.39it/s]\u001b[A\n",
            " 70%|██████▉   | 1021/1465 [00:41<00:19, 23.19it/s]\u001b[A\n",
            " 70%|██████▉   | 1025/1465 [00:41<00:16, 26.20it/s]\u001b[A\n",
            " 70%|███████   | 1029/1465 [00:41<00:16, 26.52it/s]\u001b[A\n",
            " 71%|███████   | 1033/1465 [00:41<00:16, 26.55it/s]\u001b[A\n",
            " 71%|███████   | 1037/1465 [00:41<00:15, 28.44it/s]\u001b[A\n",
            " 71%|███████   | 1041/1465 [00:42<00:16, 25.25it/s]\u001b[A\n",
            " 71%|███████▏  | 1045/1465 [00:42<00:17, 24.03it/s]\u001b[A\n",
            " 72%|███████▏  | 1049/1465 [00:42<00:15, 26.56it/s]\u001b[A\n",
            " 72%|███████▏  | 1053/1465 [00:42<00:17, 24.22it/s]\u001b[A\n",
            " 72%|███████▏  | 1057/1465 [00:42<00:16, 24.73it/s]\u001b[A\n",
            " 72%|███████▏  | 1061/1465 [00:42<00:14, 27.28it/s]\u001b[A\n",
            " 73%|███████▎  | 1065/1465 [00:42<00:15, 25.14it/s]\u001b[A\n",
            " 73%|███████▎  | 1069/1465 [00:43<00:15, 25.14it/s]\u001b[A\n",
            " 73%|███████▎  | 1073/1465 [00:43<00:16, 23.69it/s]\u001b[A\n",
            " 74%|███████▎  | 1077/1465 [00:43<00:16, 23.33it/s]\u001b[A\n",
            " 74%|███████▎  | 1080/1465 [00:43<00:15, 24.83it/s]\u001b[A\n",
            " 74%|███████▍  | 1084/1465 [00:43<00:13, 27.33it/s]\u001b[A\n",
            " 74%|███████▍  | 1087/1465 [00:43<00:15, 24.32it/s]\u001b[A\n",
            " 74%|███████▍  | 1090/1465 [00:44<00:15, 23.84it/s]\u001b[A\n",
            " 75%|███████▍  | 1093/1465 [00:44<00:15, 23.95it/s]\u001b[A\n",
            " 75%|███████▍  | 1097/1465 [00:44<00:15, 23.62it/s]\u001b[A\n",
            " 75%|███████▌  | 1101/1465 [00:44<00:14, 24.78it/s]\u001b[A\n",
            " 75%|███████▌  | 1105/1465 [00:44<00:14, 25.24it/s]\u001b[A\n",
            " 76%|███████▌  | 1109/1465 [00:44<00:13, 25.49it/s]\u001b[A\n",
            " 76%|███████▌  | 1113/1465 [00:44<00:13, 25.75it/s]\u001b[A\n",
            " 76%|███████▌  | 1116/1465 [00:45<00:13, 26.36it/s]\u001b[A\n",
            " 76%|███████▋  | 1119/1465 [00:45<00:13, 25.76it/s]\u001b[A\n",
            " 77%|███████▋  | 1122/1465 [00:45<00:13, 25.22it/s]\u001b[A\n",
            " 77%|███████▋  | 1125/1465 [00:45<00:15, 21.90it/s]\u001b[A\n",
            " 77%|███████▋  | 1129/1465 [00:45<00:14, 22.68it/s]\u001b[A\n",
            " 77%|███████▋  | 1133/1465 [00:45<00:14, 23.61it/s]\u001b[A\n",
            " 78%|███████▊  | 1137/1465 [00:45<00:13, 24.41it/s]\u001b[A\n",
            " 78%|███████▊  | 1140/1465 [00:46<00:12, 25.72it/s]\u001b[A\n",
            " 78%|███████▊  | 1144/1465 [00:46<00:13, 24.38it/s]\u001b[A\n",
            " 78%|███████▊  | 1148/1465 [00:46<00:13, 23.95it/s]\u001b[A\n",
            " 79%|███████▊  | 1152/1465 [00:46<00:13, 22.88it/s]\u001b[A\n",
            " 79%|███████▉  | 1156/1465 [00:46<00:12, 25.01it/s]\u001b[A\n",
            " 79%|███████▉  | 1160/1465 [00:46<00:12, 24.82it/s]\u001b[A\n",
            " 79%|███████▉  | 1164/1465 [00:47<00:12, 24.90it/s]\u001b[A\n",
            " 80%|███████▉  | 1168/1465 [00:47<00:12, 24.11it/s]\u001b[A\n",
            " 80%|████████  | 1172/1465 [00:47<00:11, 24.89it/s]\u001b[A\n",
            " 80%|████████  | 1176/1465 [00:47<00:11, 25.77it/s]\u001b[A\n",
            " 81%|████████  | 1180/1465 [00:47<00:11, 24.84it/s]\u001b[A\n",
            " 81%|████████  | 1184/1465 [00:47<00:11, 24.44it/s]\u001b[A\n",
            " 81%|████████  | 1188/1465 [00:47<00:10, 25.69it/s]\u001b[A\n",
            " 81%|████████▏ | 1192/1465 [00:48<00:10, 25.05it/s]\u001b[A\n",
            " 82%|████████▏ | 1196/1465 [00:48<00:10, 25.92it/s]\u001b[A\n",
            " 82%|████████▏ | 1199/1465 [00:48<00:10, 25.08it/s]\u001b[A\n",
            " 82%|████████▏ | 1202/1465 [00:48<00:11, 23.53it/s]\u001b[A\n",
            " 82%|████████▏ | 1206/1465 [00:48<00:10, 24.54it/s]\u001b[A\n",
            " 83%|████████▎ | 1210/1465 [00:48<00:10, 25.39it/s]\u001b[A\n",
            " 83%|████████▎ | 1214/1465 [00:49<00:10, 24.93it/s]\u001b[A\n",
            " 83%|████████▎ | 1218/1465 [00:49<00:09, 24.86it/s]\u001b[A\n",
            " 83%|████████▎ | 1222/1465 [00:49<00:09, 25.04it/s]\u001b[A\n",
            " 84%|████████▎ | 1226/1465 [00:49<00:09, 25.86it/s]\u001b[A\n",
            " 84%|████████▍ | 1230/1465 [00:49<00:09, 25.42it/s]\u001b[A\n",
            " 84%|████████▍ | 1234/1465 [00:49<00:09, 25.43it/s]\u001b[A\n",
            " 85%|████████▍ | 1238/1465 [00:49<00:08, 27.57it/s]\u001b[A\n",
            " 85%|████████▍ | 1242/1465 [00:50<00:08, 27.18it/s]\u001b[A\n",
            " 85%|████████▍ | 1245/1465 [00:50<00:07, 27.60it/s]\u001b[A\n",
            " 85%|████████▌ | 1248/1465 [00:50<00:09, 23.97it/s]\u001b[A\n",
            " 85%|████████▌ | 1252/1465 [00:50<00:09, 23.25it/s]\u001b[A\n",
            " 86%|████████▌ | 1256/1465 [00:50<00:08, 23.50it/s]\u001b[A\n",
            " 86%|████████▌ | 1260/1465 [00:50<00:08, 24.72it/s]\u001b[A\n",
            " 86%|████████▋ | 1264/1465 [00:50<00:07, 25.45it/s]\u001b[A\n",
            " 87%|████████▋ | 1268/1465 [00:51<00:08, 24.59it/s]\u001b[A\n",
            " 87%|████████▋ | 1272/1465 [00:51<00:08, 23.83it/s]\u001b[A\n",
            " 87%|████████▋ | 1276/1465 [00:51<00:08, 22.64it/s]\u001b[A\n",
            " 87%|████████▋ | 1280/1465 [00:51<00:07, 25.23it/s]\u001b[A\n",
            " 88%|████████▊ | 1284/1465 [00:51<00:06, 25.88it/s]\u001b[A\n",
            " 88%|████████▊ | 1288/1465 [00:51<00:07, 24.98it/s]\u001b[A\n",
            " 88%|████████▊ | 1292/1465 [00:52<00:07, 24.39it/s]\u001b[A\n",
            " 88%|████████▊ | 1296/1465 [00:52<00:06, 25.50it/s]\u001b[A\n",
            " 89%|████████▊ | 1300/1465 [00:52<00:06, 25.95it/s]\u001b[A\n",
            " 89%|████████▉ | 1304/1465 [00:52<00:06, 26.63it/s]\u001b[A\n",
            " 89%|████████▉ | 1308/1465 [00:52<00:06, 24.99it/s]\u001b[A\n",
            " 90%|████████▉ | 1312/1465 [00:52<00:06, 23.61it/s]\u001b[A\n",
            " 90%|████████▉ | 1316/1465 [00:53<00:05, 25.20it/s]\u001b[A\n",
            " 90%|█████████ | 1320/1465 [00:53<00:05, 25.72it/s]\u001b[A\n",
            " 90%|█████████ | 1324/1465 [00:53<00:05, 25.57it/s]\u001b[A\n",
            " 91%|█████████ | 1328/1465 [00:53<00:05, 26.05it/s]\u001b[A\n",
            " 91%|█████████ | 1332/1465 [00:53<00:04, 26.63it/s]\u001b[A\n",
            " 91%|█████████ | 1336/1465 [00:53<00:05, 25.62it/s]\u001b[A\n",
            " 91%|█████████▏| 1340/1465 [00:54<00:04, 25.34it/s]\u001b[A\n",
            " 92%|█████████▏| 1344/1465 [00:54<00:05, 24.12it/s]\u001b[A\n",
            " 92%|█████████▏| 1348/1465 [00:54<00:04, 24.87it/s]\u001b[A\n",
            " 92%|█████████▏| 1352/1465 [00:54<00:04, 24.52it/s]\u001b[A\n",
            " 93%|█████████▎| 1356/1465 [00:54<00:04, 26.31it/s]\u001b[A\n",
            " 93%|█████████▎| 1360/1465 [00:54<00:03, 26.31it/s]\u001b[A\n",
            " 93%|█████████▎| 1364/1465 [00:54<00:03, 28.65it/s]\u001b[A\n",
            " 93%|█████████▎| 1367/1465 [00:55<00:03, 27.76it/s]\u001b[A\n",
            " 94%|█████████▎| 1370/1465 [00:55<00:03, 26.11it/s]\u001b[A\n",
            " 94%|█████████▎| 1373/1465 [00:55<00:04, 22.52it/s]\u001b[A\n",
            " 94%|█████████▍| 1377/1465 [00:55<00:03, 23.13it/s]\u001b[A\n",
            " 94%|█████████▍| 1381/1465 [00:55<00:03, 25.25it/s]\u001b[A\n",
            " 94%|█████████▍| 1384/1465 [00:55<00:03, 25.83it/s]\u001b[A\n",
            " 95%|█████████▍| 1388/1465 [00:55<00:02, 26.09it/s]\u001b[A\n",
            " 95%|█████████▍| 1391/1465 [00:55<00:02, 24.77it/s]\u001b[A\n",
            " 95%|█████████▌| 1394/1465 [00:56<00:03, 23.28it/s]\u001b[A\n",
            " 95%|█████████▌| 1397/1465 [00:56<00:03, 22.37it/s]\u001b[A\n",
            " 96%|█████████▌| 1401/1465 [00:56<00:02, 22.53it/s]\u001b[A\n",
            " 96%|█████████▌| 1405/1465 [00:56<00:02, 23.07it/s]\u001b[A\n",
            " 96%|█████████▌| 1409/1465 [00:56<00:02, 24.11it/s]\u001b[A\n",
            " 96%|█████████▋| 1413/1465 [00:56<00:02, 24.59it/s]\u001b[A\n",
            " 97%|█████████▋| 1417/1465 [00:57<00:01, 25.16it/s]\u001b[A\n",
            " 97%|█████████▋| 1421/1465 [00:57<00:01, 26.22it/s]\u001b[A\n",
            " 97%|█████████▋| 1425/1465 [00:57<00:01, 24.61it/s]\u001b[A\n",
            " 98%|█████████▊| 1429/1465 [00:57<00:01, 24.93it/s]\u001b[A\n",
            " 98%|█████████▊| 1433/1465 [00:57<00:01, 24.50it/s]\u001b[A\n",
            " 98%|█████████▊| 1437/1465 [00:57<00:01, 25.49it/s]\u001b[A\n",
            " 98%|█████████▊| 1441/1465 [00:58<00:00, 24.58it/s]\u001b[A\n",
            " 99%|█████████▊| 1445/1465 [00:58<00:00, 26.37it/s]\u001b[A\n",
            " 99%|█████████▉| 1449/1465 [00:58<00:00, 26.41it/s]\u001b[A\n",
            " 99%|█████████▉| 1452/1465 [00:58<00:00, 26.67it/s]\u001b[A\n",
            " 99%|█████████▉| 1455/1465 [00:58<00:00, 24.10it/s]\u001b[A\n",
            "100%|█████████▉| 1459/1465 [00:58<00:00, 24.04it/s]\u001b[A\n",
            "100%|██████████| 1465/1465 [00:59<00:00, 24.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/woodnet_cm.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train_plot_and_save_224px_color.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m58",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neuralwood-Net/face-recognizer-9000/blob/main/notebooks/Train_plot_and_save_224px_color.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6k1V5shoVg"
      },
      "source": [
        "# Train networks on 224px color images\n",
        "### Notebook for training WoodNet and SqueezeNet on both the raw images and the images cropped to faces\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVI9ckKgrOFu"
      },
      "source": [
        "### Make sure the hardware is in order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1985DDBL4h",
        "outputId": "fce8d14a-1e08-4dad-edb6-e4d9a4a19bfa"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 18 09:25:09 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqcCktR3hrNG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rnSlQJGV9xh"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sys\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Placeholder to make it run until the real WoodNet is defined\n",
        "class WoodNet:\n",
        "    pass\n",
        "\n",
        "# Define an enumeration type for the different datasets\n",
        "# (easily extendable to more sets in the future)\n",
        "class Dataset:\n",
        "    RAW = 0\n",
        "    CROPPED = 1\n",
        "\n",
        "# Set what device to run the training on (preferrably GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdPDOarGkiF3"
      },
      "source": [
        "### Create configurations for the different datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-AT_GmckpFQ"
      },
      "source": [
        "# Define some configuration for the different datasets so the below code will work\n",
        "configuration = {\n",
        "    Dataset.RAW: {\n",
        "        \"blobname\": \"faces/balanced_sampled_224px_color_156240_images_70_15_15_split.zip\",\n",
        "        \"datadir\": \"sampled_dataset_balanced_244\",\n",
        "    },\n",
        "    Dataset.CROPPED: {\n",
        "        \"blobname\": \"faces/balanced_sampled_cropped_224px_color_70_15_15_split.tar.gz\",\n",
        "        \"datadir\": \"sampled_dataset_balanced_cropped_224\",\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKZU5hOmmi8e"
      },
      "source": [
        "### Decide what dataset to use for training of the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTlpcJuClI4w"
      },
      "source": [
        "# Available types are Dataset.RAW and Dataset.CROPPED\n",
        "# Choose one of them and continue\n",
        "# dataset = Dataset.RAW\n",
        "dataset = Dataset.CROPPED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gXbbcThtax"
      },
      "source": [
        "### Fetch and extract the data from the storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL7_2UEsx4IF"
      },
      "source": [
        "# Define paths for use later\n",
        "# (Kept separate from the heavy operations below in case they need to be rerun)\n",
        "BASE_PATH = \"/content\"\n",
        "\n",
        "BLOB_NAME = configuration[dataset][\"blobname\"]\n",
        "zipfilename = os.path.join(BASE_PATH, BLOB_NAME)\n",
        "extract_to_dir = os.path.join(BASE_PATH, *BLOB_NAME.split(os.path.sep)[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7t0dLiFcotJ"
      },
      "source": [
        "# Make the required directories\n",
        "os.makedirs(os.path.join(BASE_PATH, \"faces\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"checkpoints\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"logs\"), exist_ok=True)\n",
        "\n",
        "# Fetch the data\n",
        "with open(zipfilename, \"wb\") as f:\n",
        "    storage.Client.create_anonymous_client().download_blob_to_file(f\"gs://tdt4173-datasets/{BLOB_NAME}\", f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3lwkKMrOF_"
      },
      "source": [
        "# Extract the data\n",
        "if dataset.CROPPED:\n",
        "    with tarfile.open(zipfilename, \"r:gz\") as f:\n",
        "        f.extractall(extract_to_dir)\n",
        "elif dataset.RAW:\n",
        "    with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_dir)\n",
        "else:\n",
        "    raise Exception(\"Invalid dataset chosen\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNORqJDwrOGC"
      },
      "source": [
        "### Load the data into wrapper classes and apply transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgCf231_Lsw"
      },
      "source": [
        "# Custom transform to reorder the color channels to the correct order\n",
        "class BGR2RGB:\n",
        "    def __call__(self, im):\n",
        "        b, g, r = im.split()\n",
        "        return Image.merge(\"RGB\", (r, g, b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSmppOms1mel"
      },
      "source": [
        "# Set how many images to process at once\n",
        "# Can be changed to accomodate lower memory devices\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "trans = [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "]\n",
        "\n",
        "# Color channel transform only applies to cropped dataset\n",
        "if dataset.CROPPED:\n",
        "    trans = [BGR2RGB()] + trans\n",
        "\n",
        "data_transforms = transforms.Compose(trans)\n",
        "\n",
        "data_dir = os.path.join(extract_to_dir, configuration[dataset])\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms)\n",
        "                  for x in ['train', 'val', 'test']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)\n",
        "print(image_datasets['val'].classes)\n",
        "print(dataset_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrnDUfOjrOGF"
      },
      "source": [
        "### Create a helper function to aid in image plotting and show a random sample of the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfMHbGt5YbQa"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['val']))\n",
        "\n",
        "inputs, classes = inputs[:8], classes[:8]\n",
        "\n",
        "print(inputs.shape)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVzf0lAul5y"
      },
      "source": [
        "### Create a function for training and validation\n",
        "The following function trains the supplied model with the loss criterion and optimizer supplied, for the specified number of epochs. During training it logs the loss and accuracy for both training and validation. Whenever a better model is found on the validation set, the function saves the model parameters to a file for use for inference later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3jtTxxvyxT4"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    modelname = f\"{type(model).__name__}-{since}\"\n",
        "    print(f\"Training model: `{type(model).__name__}`\")\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    num_img = {\n",
        "        \"train\": 0,\n",
        "        \"val\": 0,\n",
        "    }\n",
        "    \n",
        "    datapoints_per_epoch = 100\n",
        "\n",
        "    imgs_per_datapoint = {\n",
        "        \"train\": int(float(dataset_sizes[\"train\"] / datapoints_per_epoch)),\n",
        "        \"val\": int(float(dataset_sizes[\"val\"] / datapoints_per_epoch)),\n",
        "    }\n",
        "\n",
        "    print(\"Images per phase:\", imgs_per_datapoint[\"train\"], imgs_per_datapoint[\"val\"])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 10)\n",
        "        \n",
        "        with open(os.path.join(BASE_PATH, f\"logs/{modelname}.csv\"), \"a\") as f:\n",
        "\n",
        "            # For each epoch we want to both train and evaluate in that order\n",
        "            for phase in [\"train\", \"val\"]:\n",
        "                if phase == \"train\":\n",
        "                    # Makes the network ready for training, i.e. the parameters can be tuned\n",
        "                    # and possible Dropouts are activated\n",
        "                    model.train()\n",
        "                else:\n",
        "                    # Makes the network ready for inference, i.e. it is not tunable and will\n",
        "                    # turn off regularization that might interfere with training\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                plot_loss = 0\n",
        "                plot_corrects = 0\n",
        "\n",
        "                plot_points = 0\n",
        "\n",
        "                # Iterate over training or validation data\n",
        "                for inputs, labels in tqdm(dataloaders[phase], desc=f\"Epoch: {epoch} ({phase})\", file=sys.stdout):\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # Reset the gradients before calculating new ones\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    \n",
        "                    # Ask PyTorch to generate computation graph only if in training mode\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        \n",
        "                        # Only perform update steps if we're training\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "\n",
        "                    # Save values for statistics and logging\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    plot_loss += loss.item() * inputs.size(0)\n",
        "                    plot_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    num_img[phase] += BATCH_SIZE\n",
        "                    \n",
        "                    if (num_img[phase] % imgs_per_datapoint[phase]) < (BATCH_SIZE + 1):\n",
        "                        f.write(f\"{time.time()},{epoch},{phase},\\\n",
        "                        {num_img[phase]},{plot_loss / float(imgs_per_datapoint[phase])},\\\n",
        "                        {plot_corrects / float(imgs_per_datapoint[phase])}\\n\")\n",
        "                        \n",
        "                        plot_loss = 0\n",
        "                        plot_corrects = 0\n",
        "                        plot_points += 1\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "                print(f\"Points plotted: {plot_points}\")\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == \"val\" and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    # This saves the data that can be loaded later for inference\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"loss\": epoch_loss,\n",
        "                            \"acc\": epoch_acc,\n",
        "                            \"epoch\": epoch,\n",
        "                            \"parameters\": best_model_wts,\n",
        "                        },\n",
        "                        os.path.join(BASE_PATH, f\"checkpoints/{modelname}.data\"),\n",
        "                    )\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWDUGOMqh2jh"
      },
      "source": [
        "### Prepare the home-made CNN – WoodNet\n",
        "Below are two networks. The first is made by the authors, and is made to be trained from scratch on the training data. The other is fully trained on ImageNet (1000 classes) and fine-tuned on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2JPv3bfhm4_"
      },
      "source": [
        "class WoodNet(nn.Module):\n",
        "    # After 5 pooling layers the 224x224 images are reduced to 7x7\n",
        "    # while 3 channels have become 64.\n",
        "    size_after_conv = 7 * 7 * 64\n",
        "    def __init__(self):\n",
        "        super(WoodNet, self).__init__()\n",
        "        self.features = nn.Sequential(   \n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(self.size_after_conv, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, len(class_names)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.size_after_conv)\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the net and put it on currect device (preferrably the GPU)\n",
        "woodnet = WoodNet().to(device)\n",
        "woodnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3ms2adrOGO"
      },
      "source": [
        "### Prepare the pretrained CNN – SqueezeNet\n",
        "Below is the code for loading in the pretrained SqueezeNet. After it is loaded, the last classification layer is replaced with a one with the correct amount of output classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TtQyIlYrOGO"
      },
      "source": [
        "# Load pretrained SqueezeNet\n",
        "squeezenet = models.squeezenet1_1(pretrained=True, progress=True)\n",
        "# Replace the last layer with one with the correct number of channels\n",
        "num_ftr = squeezenet.classifier[1].in_channels\n",
        "squeezenet.classifier[1] = nn.Conv2d(num_ftr, len(class_names), 1, 1)\n",
        "# Create the net and put it on currect device (preferrably the GPU)\n",
        "squeezenet = squeezenet.to(device)\n",
        "squeezenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursGj99UCtt6"
      },
      "source": [
        "### Prepare a bad net – BadNet \n",
        "Below is a simple model which will be used for performance comparison with WoodNet and SqueezeNet. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yutnVu9Ho6v"
      },
      "source": [
        "class BadNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BadNet, self).__init__()\n",
        "        \n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(224*224*3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128, len(class_names)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 224*224*3)\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "badnet = BadNet().to(device)\n",
        "badnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN9_8Ep4rOGQ"
      },
      "source": [
        "### Train the network\n",
        "Below is code that instantiates the loss function and optimization method and starts the training.\n",
        "To train every parameter in SqueezeNet, set `train_full_network = True`, and to `False` if only the last layer is to be trained. Set the variable network to the network that you want to train. Choices are `woodnet` and `squeezenet`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epKrvd6IzTeC"
      },
      "source": [
        "# network = squeezenet\n",
        "# network = badnet\n",
        "network = woodnet\n",
        "train_full_network = False\n",
        "\n",
        "if train_full_network or isinstance(network, (WoodNet, BadNet)):\n",
        "    print(\"Training full network\")\n",
        "    parameters = network.parameters()\n",
        "else:\n",
        "    print(\"Training only last layer of SqueezeNet\")\n",
        "    parameters = network.classifier[1].parameters()\n",
        "\n",
        "optimizer = torch.optim.SGD(parameters, lr=0.001, momentum=0.9)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(network, loss_function, optimizer, num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz-mn3TVrOGV"
      },
      "source": [
        "### Visualize the model performance for some images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mhJ5gmLy8CG"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WccAEZ9KrOGY"
      },
      "source": [
        "visualize_model(squeezenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wrdarsfoMY5"
      },
      "source": [
        "### Test the trained network with example images scraped from facebook etc. Can be used by uploading images and changing the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMJ4a5lX55e7"
      },
      "source": [
        "import cv2\n",
        "inputs = [\n",
        "    cv2.imread(\"/content/lars_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_3.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/ingvar_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/dwayne_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/kjartan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/faces/sampled_dataset_balanced_244/test/Kjartan/kjartan_video_5_9_augmentation_8.jpg\", cv2.IMREAD_COLOR),\n",
        "]\n",
        "\n",
        "for i, inp in enumerate(inputs):\n",
        "    inputs[i] = cv2.cvtColor(cv2.resize(inp, (244, 244)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def get_prediction_image(img, true_lab=None, plot=False):\n",
        "    assert not plot or (plot and true_lab)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    inp = cv2.resize(img, (224, 224)) / 255.0\n",
        "    inp = inp / std - mean\n",
        "    inp = inp.transpose((2, 0, 1))\n",
        "\n",
        "    imgt = torch.Tensor(inp).unsqueeze(0).to(device)\n",
        "\n",
        "    out = squeezenet(imgt)\n",
        "\n",
        "    probabilities = F.softmax(out, dim=1)\n",
        "\n",
        "    prob, class_idx = torch.max(probabilities, dim=1)\n",
        "    pred = class_names[class_idx]\n",
        "\n",
        "    if plot:\n",
        "        plt.imshow(img)\n",
        "        plt.text(5, 17,   f\"Actual   : {true_lab}\", color=\"white\", fontsize=14)\n",
        "        plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "\n",
        "    return pred, round(prob.item() * 100, 2), probabilities\n",
        "\n",
        "get_prediction_image(inputs[2], \"Morgan\", plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2c9a13X6ka0"
      },
      "source": [
        "plt.imshow(np.concatenate((inputs[1], inputs[2]), axis=1))\n",
        "(pred, prob), actual = get_prediction_image(inputs[1]), \"Morgan\"\n",
        "plt.text(5, 17,   f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "\n",
        "(pred, prob), actual = get_prediction_image(inputs[2]), \"Morgan\"\n",
        "plt.text(249, 17, f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 34, f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "plt.savefig(\"morgan_crop_plot.png\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHq7ps3PoiSu"
      },
      "source": [
        "### Code for calculating the accuracy, loss, precision, and recall for all the networks and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ABu6yANpSc"
      },
      "source": [
        "# Load saved data from a training run\n",
        "# and initialize model weights to the saved weights\n",
        "# save_data = \"/content/SqueezeNet-1605290736.1277423.data\"\n",
        "save_data = \"/content/WoodNet-1605365270.1111202_cropped.data\"\n",
        "network.load_state_dict(torch.load(save_data)[\"parameters\"])\n",
        "network = network.to(device).eval()\n",
        "\n",
        "running_loss = 0\n",
        "running_corrects = 0\n",
        "\n",
        "# Define what classes have access\n",
        "access = set([0, 1, 2])\n",
        "\n",
        "# Counters for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for inputs, labels in tqdm(dataloaders[\"test\"]):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Save values for statistics and logging\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        for pred, lab in zip(preds, labels):\n",
        "            pred, lab = pred.item(), lab.item()\n",
        "            if lab in access and pred in access:\n",
        "                tp += 1\n",
        "            elif lab in access and pred not in access:\n",
        "                fn += 1\n",
        "            elif lab not in access and pred in access:\n",
        "                fp += 1\n",
        "\n",
        "\n",
        "# Loss\n",
        "print(running_loss / dataset_sizes[\"test\"])\n",
        "\n",
        "# Accuracy\n",
        "print(running_corrects.double() / dataset_sizes[\"test\"])\n",
        "\n",
        "# Precision\n",
        "print(tp / (tp + fp))\n",
        "\n",
        "# Recall\n",
        "print(tp / (tp + fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0b5Qu2sp01"
      },
      "source": [
        "### Create confusion matrix for the WoodNet, SqueezeNet, and BadNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbHT8qJtDh8_"
      },
      "source": [
        "Below is a function which returns all true lables and a models predicted values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmqJXZNTyqCU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def get_all_preds_labels(model, loader):\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "\n",
        "    for inputs, labels in tqdm(loader):\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        preds = model(inputs)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "        all_labels = torch.cat(\n",
        "            (all_labels, labels)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds, all_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXs8fF-1Cs7"
      },
      "source": [
        "test_preds, test_labels = get_all_preds_labels(squeezenet,dataloaders[\"test\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhwawXE1s2b"
      },
      "source": [
        " #[actual,predicted]\n",
        " stacked = torch.stack(\n",
        "    (\n",
        "        test_labels\n",
        "        ,test_preds.argmax(dim=1)\n",
        "    )\n",
        "    ,dim=1\n",
        ")\n",
        "stacked.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YgRCi3sDvQm"
      },
      "source": [
        "This is a convolutional matrix in tensor form, not used for the visualisation, but we provide it as it can be useful in some cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XToO6Pq1KJ0"
      },
      "source": [
        "def cm_tensor(stacked):\n",
        "  cm_tensor = torch.zeros(4,4, dtype=torch.int64)\n",
        "  for pair in stacked:\n",
        "    al,pl = pair.tolist()\n",
        "    cm_tensor[int(al),int(pl)] = cm_tensor[int(al),int(pl)] + 1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdpyIDKh3HCW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEGfOdRhD3r5"
      },
      "source": [
        "The plotting is done on the cpu. As such we return our calculated test predictions and acutal labels to be calculated on the cpu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS7LsSON5E8w"
      },
      "source": [
        "\n",
        "test_labels = test_labels.cpu()\n",
        "test_preds = test_preds.cpu()\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds.argmax(dim=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS7vIhH0EDxA"
      },
      "source": [
        "The below plotter is based on the code from DeepLizard: https://deeplizard.com/learn/video/0LhiS6yu2qQ?fbclid=IwAR1Zb3LSBe4nhuxa6OhwpW4-rXwg7LhMIeG0C0iCWMrYLH2Bkhfh-z5IaL0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrZWza7pBfYY"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, filename, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "        pass\n",
        "\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "# plot_confusion_matrix(cm,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_zuCWeJDAm0"
      },
      "source": [
        "#test_preds, test_labels = get_all_preds_labels(model,dataloader_and_type)\n",
        "def cnn_constructor(preds,labels,filename,title): \n",
        "    stacked = torch.stack(\n",
        "      (\n",
        "          labels\n",
        "          ,preds.argmax(dim=1)\n",
        "      )\n",
        "      ,dim=1\n",
        "    )\n",
        "    labels = labels.cpu()\n",
        "    preds = preds.cpu()\n",
        "    cm = confusion_matrix(labels, preds.argmax(dim=1))\n",
        "    plot_confusion_matrix(cm,class_names,filename,title=title)\n",
        "#SqueezeNet\n",
        "#test_preds_s,test_labels_s=get_all_preds_labels(squeezenet,dataloaders[\"test\"])\n",
        "#cnn_constructor(test_preds_s,test_labels_s,\"/content/squeezenet_cm.png\",title=\"SqueezeNet Confusion Matrix\");\n",
        "#WoodNet\n",
        "test_preds_w,test_labels_w = get_all_preds_labels(woodnet,dataloaders[\"test\"])\n",
        "cnn_constructor(test_preds_w,test_labels_w,\"/content/woodnet_cm.png\",title=\"WoodNet Confusion Matrix\");\n",
        "\n",
        "# test_preds,test_labels = get_all_preds_labels(badnet,dataloaders[\"test\"])\n",
        "#cnn_constructor(test_preds,test_labels,\"/content/badnet_cm.png\",title=\"BadNet Confusion Matrix\");\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
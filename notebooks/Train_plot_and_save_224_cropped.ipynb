{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neuralwood-Net/face-recognizer-9000/blob/main/notebooks/Train_plot_and_save_cropped_224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6k1V5shoVg"
      },
      "source": [
        "# Train networks on 224px color images cropped to face\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVI9ckKgrOFu"
      },
      "source": [
        "### Make sure the hardware is in order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1985DDBL4h",
        "outputId": "b2d01d2b-f3ba-4a9a-9932-c926a7656d87"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqcCktR3hrNG"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0rnSlQJGV9xh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Placeholder to make it run until the real WoodNet is defined\n",
        "class WoodNet:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4gqZD7ulPLt",
        "outputId": "fd9a6ce1-46cd-4f10-e394-77fb8875f3c0"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gXbbcThtax"
      },
      "source": [
        "### Fetch and extract the data from the storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zL7_2UEsx4IF"
      },
      "outputs": [],
      "source": [
        "# Define paths separate from the heavy operations below\n",
        "BASE_PATH = \"/content\"\n",
        "\n",
        "BLOB_NAME = \"faces/balanced_sampled_cropped_224px_color_70_15_15_split.tar.gz\"\n",
        "zipfilename = os.path.join(BASE_PATH, BLOB_NAME)\n",
        "extract_to_dir = os.path.join(BASE_PATH, *BLOB_NAME.split(os.path.sep)[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T7t0dLiFcotJ"
      },
      "outputs": [],
      "source": [
        "# Fetch the data\n",
        "from google.cloud import storage\n",
        "\n",
        "# Make the required directories\n",
        "os.makedirs(os.path.join(BASE_PATH, \"faces\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"checkpoints\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_PATH, \"logs\"), exist_ok=True)\n",
        "\n",
        "with open(zipfilename, \"wb\") as f:\n",
        "    storage.Client.create_anonymous_client().download_blob_to_file(f\"gs://tdt4173-datasets/{BLOB_NAME}\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3d3lwkKMrOF_"
      },
      "outputs": [],
      "source": [
        "# Extract the data\n",
        "f = tarfile.open(zipfilename, \"r:gz\")\n",
        "f.extractall(\"/content/faces\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNORqJDwrOGC"
      },
      "source": [
        "### Load the data into wrapper classes and apply normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9wgCf231_Lsw"
      },
      "outputs": [],
      "source": [
        "class BGR2RGB:\n",
        "    def __call__(self, im):\n",
        "\n",
        "        b, g, r = im.split()\n",
        "\n",
        "        return Image.merge(\"RGB\", (r, g, b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSmppOms1mel",
        "outputId": "67d28291-9469-4272-d3ac-43acec7d37e4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    BGR2RGB(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "data_dir = os.path.join(extract_to_dir, \"sampled_dataset_balanced_cropped_224\")\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms)\n",
        "                  for x in ['train', 'val', 'test']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)\n",
        "print(image_datasets['val'].classes)\n",
        "print(dataset_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrnDUfOjrOGF"
      },
      "source": [
        "### Create a helper function to aid in image plotting and show a random sample of the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "SfMHbGt5YbQa",
        "outputId": "5b8de174-65d0-4c4e-fa39-204c7cbc04b0"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['val']))\n",
        "\n",
        "inputs, classes = inputs[:8], classes[:8]\n",
        "\n",
        "print(inputs.shape)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVzf0lAul5y"
      },
      "source": [
        "### Create a function for training and validation\n",
        "The following function trains the supplied model with the loss criterion and optimizer supplied, for the specified number of epochs. During training it logs the loss and accuracy for both training and validation. Whenever a better model is found on the validation set, the function saves the model parameters to a file for use for inference later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_3jtTxxvyxT4"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    modelname = f\"{type(model).__name__}-{since}\"\n",
        "    print(f\"Training model: `{type(model).__name__}`\")\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    num_img = {\n",
        "        \"train\": 0,\n",
        "        \"val\": 0,\n",
        "    }\n",
        "    \n",
        "    datapoints_per_epoch = 100\n",
        "\n",
        "    imgs_per_datapoint = {\n",
        "        \"train\": int(float(dataset_sizes[\"train\"] / datapoints_per_epoch)),\n",
        "        \"val\": int(float(dataset_sizes[\"val\"] / datapoints_per_epoch)),\n",
        "    }\n",
        "\n",
        "    print(\"Images per phase:\", imgs_per_datapoint[\"train\"], imgs_per_datapoint[\"val\"])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 10)\n",
        "        \n",
        "        with open(os.path.join(BASE_PATH, f\"logs/{modelname}.csv\"), \"a\") as f:\n",
        "\n",
        "            # For each epoch we want to both train and evaluate in that order\n",
        "            for phase in [\"train\", \"val\"]:\n",
        "                if phase == \"train\":\n",
        "                    # Makes the network ready for training, i.e. the parameters can be tuned\n",
        "                    # and possible Dropouts are activated\n",
        "                    model.train()\n",
        "                else:\n",
        "                    # Makes the network ready for inference, i.e. it is not tunable and will\n",
        "                    # turn off regularization that might interfere with training\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                plot_loss = 0\n",
        "                plot_corrects = 0\n",
        "\n",
        "                plot_points = 0\n",
        "\n",
        "                # Iterate over training or validation data\n",
        "                for inputs, labels in tqdm(dataloaders[phase], desc=f\"Epoch: {epoch} ({phase})\", file=sys.stdout):\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # Reset the gradients before calculating new ones\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    \n",
        "                    # Ask PyTorch to generate computation graph only if in training mode\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        \n",
        "                        # Only perform update steps if we're training\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "\n",
        "                    # Save values for statistics and logging\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    plot_loss += loss.item() * inputs.size(0)\n",
        "                    plot_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                    num_img[phase] += BATCH_SIZE\n",
        "                    \n",
        "                    if (num_img[phase] % imgs_per_datapoint[phase]) < (BATCH_SIZE + 1):\n",
        "                        f.write(f\"{time.time()},{epoch},{phase},\\\n",
        "                        {num_img[phase]},{plot_loss / float(imgs_per_datapoint[phase])},\\\n",
        "                        {plot_corrects / float(imgs_per_datapoint[phase])}\\n\")\n",
        "                        \n",
        "                        plot_loss = 0\n",
        "                        plot_corrects = 0\n",
        "                        plot_points += 1\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "                print(f\"Points plotted: {plot_points}\")\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == \"val\" and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"loss\": epoch_loss,\n",
        "                            \"acc\": epoch_acc,\n",
        "                            \"epoch\": epoch,\n",
        "                            \"parameters\": best_model_wts,\n",
        "                        },\n",
        "                        os.path.join(BASE_PATH, f\"checkpoints/{modelname}.data\"),\n",
        "                    )\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWDUGOMqh2jh"
      },
      "source": [
        "### Prepare the home-made CNN – WoodNet\n",
        "Below is two networks. The first is made by the authors, and is made to be trained from scratch on the training data. The other is fully trained on ImageNet (1000 classes) and fine-tuned on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2JPv3bfhm4_",
        "outputId": "00fec287-87af-49ea-88e5-434a80e5aa54"
      },
      "outputs": [],
      "source": [
        "class WoodNet(nn.Module):\n",
        "    size_after_conv = 7 * 7 * 64\n",
        "    def __init__(self):\n",
        "        super(WoodNet, self).__init__()\n",
        "        self.features = nn.Sequential(   \n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(self.size_after_conv, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, len(class_names)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.size_after_conv)\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "woodnet = WoodNet()\n",
        "print(woodnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3ms2adrOGO"
      },
      "source": [
        "### Prepare the pretrained CNN – SqueezeNet\n",
        "Below is the code for loading in the pretrained SqueezeNet. After it is loaded, the last classification layer is replaced with a one with the correct amount of output classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TtQyIlYrOGO",
        "outputId": "f12ec5e5-6ee8-48c9-86f3-b1fec4410d08"
      },
      "outputs": [],
      "source": [
        "squeezenet = models.squeezenet1_1(pretrained=True, progress=True)\n",
        "num_ftr = squeezenet.classifier[1].in_channels\n",
        "squeezenet.classifier[1] = nn.Conv2d(num_ftr, len(class_names), 1, 1)\n",
        "squeezenet = squeezenet.to(device)\n",
        "squeezenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN9_8Ep4rOGQ"
      },
      "source": [
        "### Train the network\n",
        "Below is code that instantiates the loss function and optimization method and starts the training.\n",
        "To train every parameter in SqueezeNet, set `train_full_network = True`, and to `False` if only the last layer is to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epKrvd6IzTeC",
        "outputId": "42cc27fe-20bf-40ca-d966-fe966e6756f1"
      },
      "outputs": [],
      "source": [
        "# network = squeezenet\n",
        "network = woodnet\n",
        "train_full_network = False\n",
        "\n",
        "if train_full_network or isinstance(network, WoodNet):\n",
        "    print(\"Training full network\")\n",
        "    parameters = network.parameters()\n",
        "else:\n",
        "    print(\"Training only last layer of SqueezeNet\")\n",
        "    parameters = network.classifier[1].parameters()\n",
        "\n",
        "optimizer = torch.optim.SGD(parameters, lr=0.001, momentum=0.9)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(network, loss_function, optimizer, num_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5F-eC4VSne"
      },
      "source": [
        "### Upload model weights and training logs to storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hzPXLQkXjI",
        "outputId": "4ce2523c-8f6f-449c-d2e6-84c1a376af4c"
      },
      "outputs": [],
      "source": [
        "!ls /content/checkpoints -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suEXstCJQzsv"
      },
      "outputs": [],
      "source": [
        "# Upload checkpoints to storage\n",
        "client = storage.Client.from_service_account_json(\"/content/drive/My Drive/## Project/TDT4173 Deep Learning Project-91d3b469375c.json\")\n",
        "bucket = client.get_bucket(\"tdt4173-datasets\")\n",
        "\n",
        "blob = bucket.blob(\"checkpoints/SqueezeNet-1605361529.9021263_cropped.data\")\n",
        "filename = \"/content/checkpoints/SqueezeNet-1605361529.9021263.data\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"checkpoints/WoodNet-1605365270.1111202_cropped.data\")\n",
        "filename = \"/content/checkpoints/WoodNet-1605365270.1111202.data\"\n",
        "blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "3pexLiMsU-A2",
        "outputId": "460a1767-0890-4abb-c7d3-48e0ecb6248b"
      },
      "outputs": [],
      "source": [
        "# Upload logs to storage\n",
        "blob = bucket.blob(\"logs/SqueezeNet-1605290215.097698.csv\")\n",
        "filename = \"/content/logs/SqueezeNet-1605290215.097698.csv\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"logs/SqueezeNet-1605290736.1277423.csv\")\n",
        "filename = \"/content/logs/SqueezeNet-1605290736.1277423.csv\"\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "blob = bucket.blob(\"logs/WoodNet-1605294933.5362356.csv\")\n",
        "filename = \"/content/logs/WoodNet-1605294933.5362356.csv\"\n",
        "blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz-mn3TVrOGV"
      },
      "source": [
        "### Visualize the model performance for some images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mhJ5gmLy8CG"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "WccAEZ9KrOGY",
        "outputId": "b9f83f2d-bc39-45e6-d29e-7c74c8675cc2"
      },
      "outputs": [],
      "source": [
        "visualize_model(squeezenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEbsDuXG97Nd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "bMJ4a5lX55e7",
        "outputId": "4c4d0ad5-7c03-4310-d07f-7285bac81cc6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "inputs = [\n",
        "    cv2.imread(\"/content/lars_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/morgan_3.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/ingvar_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/dwayne_1.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/kjartan_2.png\", cv2.IMREAD_COLOR),\n",
        "    cv2.imread(\"/content/faces/sampled_dataset_balanced_244/test/Kjartan/kjartan_video_5_9_augmentation_8.jpg\", cv2.IMREAD_COLOR),\n",
        "]\n",
        "\n",
        "for i, inp in enumerate(inputs):\n",
        "    inputs[i] = cv2.cvtColor(cv2.resize(inp, (244, 244)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def get_prediction_image(img, true_lab=None, plot=False):\n",
        "    assert not plot or (plot and true_lab)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    inp = cv2.resize(img, (224, 224)) / 255.0\n",
        "    inp = inp / std - mean\n",
        "    inp = inp.transpose((2, 0, 1))\n",
        "\n",
        "    imgt = torch.Tensor(inp).unsqueeze(0).to(device)\n",
        "\n",
        "    out = squeezenet(imgt)\n",
        "\n",
        "    probabilities = F.softmax(out, dim=1)\n",
        "\n",
        "    prob, class_idx = torch.max(probabilities, dim=1)\n",
        "    pred = class_names[class_idx]\n",
        "\n",
        "    if plot:\n",
        "        plt.imshow(img)\n",
        "        plt.text(5, 17,   f\"Actual   : {true_lab}\", color=\"white\", fontsize=14)\n",
        "        plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "\n",
        "    return pred, round(prob.item() * 100, 2), probabilities\n",
        "\n",
        "get_prediction_image(inputs[2], \"Morgan\", plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "n2c9a13X6ka0",
        "outputId": "c21b1f06-55d0-4c60-e320-7a2ea6824673"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.concatenate((inputs[1], inputs[2]), axis=1))\n",
        "(pred, prob), actual = get_prediction_image(inputs[1]), \"Morgan\"\n",
        "plt.text(5, 17,   f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 34,   f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(5, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "\n",
        "(pred, prob), actual = get_prediction_image(inputs[2]), \"Morgan\"\n",
        "plt.text(249, 17, f\"Actual   : {actual}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 34, f\"Predicted: {pred}\", color=\"white\", fontsize=14)\n",
        "plt.text(249, 52,   f\"[Certainty ({prob}%)]\", color=\"white\", fontsize=12)\n",
        "\n",
        "plt.savefig(\"morgan_crop_plot.png\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ABu6yANpSc",
        "outputId": "50f00396-a2a7-42c2-a027-4b34b99a598e"
      },
      "outputs": [],
      "source": [
        "woodnet.load_state_dict(torch.load(\"/content/WoodNet-1605365270.1111202_cropped.data\")[\"parameters\"])\n",
        "\n",
        "model = woodnet.to(device).eval()\n",
        "\n",
        "running_loss = 0\n",
        "running_corrects = 0\n",
        "\n",
        "access = set([0, 1, 2])\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "\n",
        "for inputs, labels in tqdm(dataloaders[\"test\"]):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Save values for statistics and logging\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        for pred, lab in zip(preds, labels):\n",
        "            pred, lab = pred.item(), lab.item()\n",
        "            if lab in access and pred in access:\n",
        "                tp += 1\n",
        "            elif lab in access and pred not in access:\n",
        "                fn += 1\n",
        "            elif lab not in access and pred in access:\n",
        "                fp += 1\n",
        "\n",
        "\n",
        "print(running_loss / dataset_sizes[\"test\"], running_corrects.double() / dataset_sizes[\"test\"])\n",
        "\n",
        "# Precision\n",
        "print(tp / (tp + fp))\n",
        "\n",
        "# Recall\n",
        "print(tp / (tp + fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um68jE4TWOen"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Train_plot_and_save_cropped_224.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m58",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
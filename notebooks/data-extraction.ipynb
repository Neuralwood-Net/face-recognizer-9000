{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgaug in /opt/conda/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug) (1.7.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug) (1.15.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug) (0.17.2)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug) (2.9.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug) (3.3.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug) (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from imgaug) (1.18.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug) (1.5.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug) (7.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (2.5)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug) (2.8.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug\n",
    "!pip install tensorflow-gpu\n",
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.image import crop_to_bounding_box\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/jupyter/data/faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Connect to google cloud storage\n",
    "client = storage.Client()\n",
    "\n",
    "bucket_name = \"tdt4173-datasets\"\n",
    "bucket = client.get_bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download videos from google cloud storage\n",
    "DATASET = \"faces/videos/faces_final.zip\"\n",
    "FILEPATH = \"/home/jupyter/data/faces/faces_final.zip\"\n",
    "blob = bucket.get_blob(DATASET)\n",
    "blob.download_to_filename(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_by_bounding_box(img, detector, vertical_expand=0.05, draw=False):\n",
    "    result = detector.detect_faces(img)\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    bounding_box = result[0]['box']\n",
    "    \n",
    "    if draw:\n",
    "        before = img.copy()\n",
    "        cv2.rectangle(img,\n",
    "                  (bounding_box[0], bounding_box[1]),\n",
    "                  (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n",
    "                  (255,255,255),\n",
    "                  10)\n",
    "\n",
    "    y = bounding_box[1]\n",
    "    x = bounding_box[0]\n",
    "    height = bounding_box[3]\n",
    "    width = bounding_box[2]\n",
    "\n",
    "    # Account for top left corner outside image\n",
    "    if y < 0:\n",
    "        height += y\n",
    "        y = 0\n",
    "    if x < 0:\n",
    "        width += x\n",
    "        x = 0\n",
    "\n",
    "    # Expand bounding box vertically by vertical_expand % in each direction\n",
    "    # Move y-coord of top-left corner upwards\n",
    "    old_y = y\n",
    "    y = max(y - int(vertical_expand * height), 0)\n",
    "    height += (old_y - y)\n",
    "\n",
    "    # Increase height of bounding box\n",
    "    h, w, _ = img.shape\n",
    "    height = min(int((1 + vertical_expand) * height), h)\n",
    "\n",
    "    # Fill horizontally or vertically until width = height\n",
    "    if height > width:\n",
    "        if height > w:\n",
    "            x = 0\n",
    "            width = w\n",
    "        else:\n",
    "            diff_to_fill = height - width\n",
    "\n",
    "            # Fill leftwards\n",
    "            old_x = x\n",
    "            x = max(0, x - int(diff_to_fill / 2))\n",
    "            x_change = (old_x - x)\n",
    "            diff_to_fill -= x_change\n",
    "            width += x_change\n",
    "\n",
    "            # Fill rightwards\n",
    "            width = min(width + diff_to_fill, w - x)\n",
    "    else:\n",
    "        diff_to_fill = width - height\n",
    "        # Fill upwards\n",
    "        old_y = y\n",
    "        y = max(0, y - int(diff_to_fill/2))\n",
    "        y_change = (old_y - y)\n",
    "        diff_to_fill -= y_change\n",
    "        height += y_change\n",
    "\n",
    "        # Fill downwards\n",
    "        height += min(height + diff_to_fill, h - y)\n",
    "    # Cut height if larger than width\n",
    "    if height > w:\n",
    "        diff = height - w\n",
    "        y += int(diff/2)\n",
    "        height -= diff\n",
    "\n",
    "    crop = crop_to_bounding_box(img, y, x, height, width).numpy()\n",
    "\n",
    "    if draw: \n",
    "        images = [before, img, crop]\n",
    "        plt.imshow(ia.draw_grid(images, cols=len(images), rows=1))\n",
    "    return crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract images from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img, img_size=None, gray=False, crop=False, rotate=False):\n",
    "   \n",
    "    # Rotate image 270 degrees to the right\n",
    "    if rotate:\n",
    "        img = np.rot90(img, k=3)\n",
    "    \n",
    "    if gray:\n",
    "        # Convert to grayscale\n",
    "        # Note: we convert from BGR as VideoCapture \n",
    "        # converts the images to BGR color frame by default\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "    # Crop to square image\n",
    "    if crop:\n",
    "        h, w, _ = img.shape\n",
    "        crop = iaa.CropToFixedSize(height=min([h, w]), width=min([h,w]), position='center')\n",
    "        img = crop(image=img)\n",
    "    \n",
    "    if img_size is not None:\n",
    "        # Resize image to specified size\n",
    "        #img = cv2.resize(img, (img_size, img_size)) / 255.0\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        if gray:\n",
    "            img = np.reshape(img, (img_size, img_size, 1))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def face_det_process(img, face=True, img_size=64):\n",
    "   \n",
    "    # Rotate image 270 degrees to the right\n",
    "    img = np.rot90(img, k=3)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    # Note: we convert from BGR as VideoCapture \n",
    "    # converts the images to BGR color frame by default\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if not face:\n",
    "        # Crop to square image\n",
    "        h, w, _ = img.shape\n",
    "        crop = iaa.CropToFixedSize(height=min([h, w]), width=min([h,w]), position='center')\n",
    "        img = crop(image=img)\n",
    "    \n",
    "    else:        \n",
    "        # Find face and crop around\n",
    "        crop = crop_by_bounding_box(img, detector, vertical_expand=0.1, draw=False)\n",
    "        if crop is None:\n",
    "            h, w, _ = img.shape\n",
    "            crop_aug = iaa.CropToFixedSize(height=min([h, w]), width=min([h,w]), position='center')\n",
    "            img = crop_aug(image=img)\n",
    "        else:\n",
    "            img = crop\n",
    "    \n",
    "    # Resize image to specified size\n",
    "    #img = cv2.resize(img, (img_size, img_size)) / 255.0\n",
    "    #img = cv2.resize(img, (img_size, img_size))\n",
    "    #img = np.reshape(img, (img_size, img_size, 1))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_label(name, folder):\n",
    "    # Find maximum label of files named as \"name_##.jpg\" in directory\n",
    "    highest = 0\n",
    "    if not os.path.isdir(folder):\n",
    "        return highest\n",
    "    for file in os.listdir(folder):\n",
    "        if name.lower() not in file:\n",
    "            continue\n",
    "        label = file.rpartition(os.sep)[2].split(\".\")[0].rpartition(\"_\")[2]\n",
    "        highest = max(highest, int(label))\n",
    "    return highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_video(video, target_dir, file_prefix, img_size=64, processing_func=preprocess_image, silent=False):    \n",
    "    \"\"\"\n",
    "    Saves every single frame of a video as images.\n",
    "    Converts to greyscale, pads with black to make square images and resizes.\n",
    "    \"\"\"    \n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    \n",
    "    video_name = video.rpartition(os.sep)[2].split(\".\")[0]\n",
    "    \n",
    "    # Where to save images    \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    prev_label = max_label(file_prefix, target_dir)\n",
    "    label = prev_label\n",
    "    face = not video_name.startswith(\"not_face\")\n",
    "    \n",
    "    while True:\n",
    "        # Read image from video\n",
    "        success, image = vidcap.read()\n",
    "        \n",
    "        # if frame is read correctly, succes is True\n",
    "        if not success:\n",
    "            if not silent:\n",
    "                print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            return label - prev_label\n",
    "        \n",
    "        # Convert to greyscale, make square and resize\n",
    "        image = processing_func(image, face)\n",
    "\n",
    "        # Save to file\n",
    "        label += 1\n",
    "        file_name = f\"{file_prefix}_{str(label)}.jpg\"\n",
    "        path = os.path.join(target_dir, file_name)\n",
    "        cv2.imwrite(path, image)\n",
    "\n",
    "        # Check that image is not corrupted\n",
    "        if cv2.imread(path) is None:\n",
    "            print(f\"WARNING: image corrupted at path {path}\")\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            if not silent:\n",
    "                print(f'Image successfully written at {path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [33:05<00:00, 283.66s/it]\n",
      "100%|██████████| 20/20 [1:10:08<00:00, 210.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kjartan: 1953, Lars: 0, Morgan: 0, Other: 5599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "KJARTAN = BASE_PATH + \"videos/Kjartan\"\n",
    "LARS = BASE_PATH + \"videos/Lars\"\n",
    "MORGAN = BASE_PATH + \"videos/Morgan\"\n",
    "OTHER = BASE_PATH + \"videos/Other\"\n",
    "\n",
    "LABELS = {LARS: 0, MORGAN: 1, KJARTAN: 2, OTHER: 3}\n",
    "counts = {KJARTAN: 0, LARS: 0, MORGAN: 0, OTHER: 0}\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "# For each class, extract all frames from all videos, preprocess image and save to target_dir\n",
    "for label in LABELS:\n",
    "    if label in [LARS, MORGAN]: # Skip the first to classes, already done\n",
    "        continue\n",
    "    count = 0\n",
    "    video_num = 0\n",
    "    for video in tqdm(os.listdir(label)):\n",
    "        if not video.endswith(\".MOV\"):\n",
    "            continue\n",
    "        # Extract each frame of video, preprocess and save to directory\n",
    "        name = label.rpartition(\"/\")[2]\n",
    "        target_dir = f\"/home/jupyter/data/faces/images/crop_no_resize/{name}\"\n",
    "        video_path = os.path.join(label, video)\n",
    "        count += extract_images_from_video(video_path, target_dir, file_prefix=name.lower()+f\"_video_{video_num}\", processing_func=face_det_process, img_size=IMAGE_SIZE, silent=True)\n",
    "        video_num+=1\n",
    "\n",
    "    counts[label] = count\n",
    "    \n",
    "\n",
    "print(f\"Kjartan: {counts[KJARTAN]}, Lars: {counts[LARS]}, Morgan: {counts[MORGAN]}, Other: {counts[OTHER]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and sample images\n",
    "- At this point we have all cropped images in their original square size in `images/crop_no_resize`\n",
    "- Now we resize and sample the images before augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars 39060\n",
      "Morgan 39080\n",
      "Kjartan 39060\n",
      "Other 39080\n",
      "Minimum sample size:  39060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Sampling and resizing {min_sample_size} images from each class to {target}...\")\\nfor label in LABELS:\\n    sample = np.random.choice(images[label], min_sample_size, replace=False)\\n    target_path = target + label\\n    for image_path in tqdm(sample):  \\n        if not image_path.endswith(\".jpg\"):\\n            continue\\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\\n        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\\n        os.makedirs(target_path, exist_ok=True)\\n        cv2.imwrite(os.path.join(target_path, image_path.rpartition(os.sep)[2]), image)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KJARTAN = \"Kjartan\"\n",
    "LARS = \"Lars\"\n",
    "MORGAN = \"Morgan\"\n",
    "OTHER = \"Other\"\n",
    "\n",
    "LABELS = {LARS: 0, MORGAN: 1, KJARTAN: 2, OTHER: 3}\n",
    "\n",
    "path = BASE_PATH + \"images/crop_no_resize/\"\n",
    "target = BASE_PATH + \"balanced_images/sampling_crop_224/\"\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "images = {}\n",
    "sizes = []\n",
    "\n",
    "for label in LABELS:\n",
    "    images[label] = []\n",
    "    for image_file in os.listdir(path + label):\n",
    "        images[label].append(os.path.join(path + label, image_file))\n",
    "    sizes.append(len(images[label]))\n",
    "    print(label, len(images[label]))\n",
    "min_sample_size = min(sizes)\n",
    "print(\"Minimum sample size: \", min_sample_size)\n",
    "\n",
    "print(f\"Sampling and resizing {min_sample_size} images from each class to {target}...\")\n",
    "for label in LABELS:\n",
    "    sample = np.random.choice(images[label], min_sample_size, replace=False)\n",
    "    target_path = target + label\n",
    "    for image_path in tqdm(sample):  \n",
    "        if not image_path.endswith(\".jpg\"):\n",
    "            continue\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "        cv2.imwrite(os.path.join(target_path, image_path.rpartition(os.sep)[2]), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, num_augmentations=19):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(-5, 5)),\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255)), # crop images by -5% to 10% of their height/width\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 20)), \n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)), # Blur images using a gaussian kernel with sigma between 0.0 and 1.0.\n",
    "        iaa.Add((-10,10)), # change brightness of images (by -10 to 10 of original value)\n",
    "        iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, cval=(0,255)) # translate by -10 to +10 percent (per axis)\n",
    "    ], random_order=True)\n",
    "\n",
    "    \n",
    "    images_aug = [seq(image=image) for _ in range(num_augmentations)] # random order is sampled once per batch, and not once per image in the batch\n",
    "    return images_aug\n",
    "\n",
    "def augment_and_save(image_path, target_dir, num_augmentations=19, silent=False):\n",
    "    # Read image from file\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    # Generate augmentations\n",
    "    augmentations = augment_image(image, num_augmentations)\n",
    "    augmentations.insert(0, image)\n",
    "\n",
    "    count = len(augmentations)\n",
    "    \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Save augmentations to file\n",
    "    for i, img in enumerate(augmentations):\n",
    "        path = os.path.join(target_dir, image_path.rpartition(os.sep)[2].split(\".\")[0]) + f\"_augmentation_{i}.jpg\"\n",
    "        cv2.imwrite(path, img)\n",
    "        \n",
    "        # Check that image is not corrupted\n",
    "        if cv2.imread(path) is None:\n",
    "            print(f\"WARNING: image corrupted at path {path}\")\n",
    "            os.remove(path)\n",
    "            count -=1\n",
    "            count = 0 if count < 0 else count\n",
    "        else:\n",
    "            if not silent:\n",
    "                print(f'Image successfully written at {path}')\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1953/1953 [05:58<00:00,  5.45it/s]\n",
      " 99%|█████████▉| 1935/1954 [05:52<00:03,  5.44it/s]"
     ]
    }
   ],
   "source": [
    "KJARTAN = \"Kjartan\"\n",
    "LARS = \"Lars\"\n",
    "MORGAN = \"Morgan\"\n",
    "OTHER = \"Other\"\n",
    "\n",
    "LABELS = {LARS: 0, MORGAN: 1, KJARTAN: 2, OTHER: 3}\n",
    "counts = {KJARTAN: 0, LARS: 0, MORGAN: 0, OTHER: 0}\n",
    "N_AUGMENTATIONS = 19\n",
    "\n",
    "# For each processed image, generate n augmentations and save to augmented_images\n",
    "for label in LABELS:\n",
    "    if label == MORGAN:\n",
    "        continue\n",
    "    count = 0\n",
    "    path = f\"{BASE_PATH}balanced_images/sampling_crop_224/{label}\"\n",
    "    for image in tqdm(os.listdir(path)):\n",
    "        if not image.endswith(\".jpg\"):\n",
    "            continue\n",
    "        image_path = os.path.join(path, image)\n",
    "        target_dir = f\"{BASE_PATH}augmented_images/sampling_crop_224/{label}\"\n",
    "        count += augment_and_save(image_path, target_dir, num_augmentations=N_AUGMENTATIONS, silent=True)\n",
    "    counts[label] += count\n",
    "    \n",
    "\n",
    "print(f\"Kjartan: {counts[KJARTAN]}, Lars: {counts[LARS]}, Morgan: {counts[MORGAN]}, Other: {counts[OTHER]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62460/62460 [00:00<00:00, 518954.92it/s]\n",
      "  0%|          | 33/39060 [00:00<01:58, 328.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars 62460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39060/39060 [06:43<00:00, 96.73it/s] \n",
      "100%|██████████| 63360/63360 [00:00<00:00, 522053.04it/s]\n",
      "  0%|          | 14/39060 [00:00<05:55, 109.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morgan 63360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39060/39060 [11:17<00:00, 57.65it/s] \n",
      "100%|██████████| 39060/39060 [00:00<00:00, 539918.12it/s]\n",
      "  0%|          | 2/39060 [00:00<35:06, 18.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kjartan 39060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39060/39060 [15:16<00:00, 42.63it/s]\n",
      "100%|██████████| 111980/111980 [00:00<00:00, 536084.57it/s]\n",
      "  0%|          | 3/39060 [00:00<27:10, 23.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other 111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39060/39060 [16:24<00:00, 39.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "path = BASE_PATH + \"augmented_images/sampling_crop_224/\"\n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "\n",
    "print(\"Collecting images...\")\n",
    "for label in LABELS:\n",
    "    for image_file in tqdm(os.listdir(path + label)):\n",
    "        image_path = os.path.join(path + label, image_file)\n",
    "        training_data.append(image_path)\n",
    "        training_labels.append(label)\n",
    "\n",
    "\n",
    "X = np.array(training_data)\n",
    "y = np.array(training_labels)\n",
    "\n",
    "# 70/15/15 train/val/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_train.shape) # Should be (n, 224, 224, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109368it [25:04, 72.71it/s] \n",
      "23436it [08:21, 46.74it/s]\n",
      "23436it [08:27, 46.15it/s]\n"
     ]
    }
   ],
   "source": [
    "target = f\"{BASE_PATH}sampled_dataset_balanced_cropped_224/\"\n",
    "print(f\"Copying images from {path} to {target}\")\n",
    "\n",
    "dataset = {\"train\": (X_train, y_train), \"val\": (X_val, y_val), \"test\": (X_test, y_test)}\n",
    "for mode in dataset:\n",
    "    path = os.path.join(target, mode)\n",
    "    images, labels = dataset[mode]\n",
    "    for i, image_path in tqdm(enumerate(images)):\n",
    "        target_dir = os.path.join(path, labels[i])\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        shutil.copy(image_path, target_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress images and upload to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/jupyter/data/faces/images && sudo zip -rq balanced_sampled_cropped_224px_color_70_15_15_split.zip sampled_dataset_balanced_cropped_224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_directory = \"faces/balanced_sampled_cropped_224px_color_70_15_15_split.zip\"\n",
    "blob = bucket.blob(cloud_directory)\n",
    "\n",
    "source_file_name = \"/balanced_sampled_cropped_224px_color_70_15_15_split.zip.zip\"\n",
    "blob.upload_from_filename(source_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tensor from augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29100/29100 [00:00<00:00, 410842.24it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 440647.52it/s]\n",
      "100%|██████████| 50861/50861 [00:00<00:00, 461310.31it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 110120/110120 [00:13<00:00, 8464.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving torch object to file\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "KJARTAN = BASE_PATH + \"augmented_images/Kjartan\"\n",
    "LARS = BASE_PATH + \"augmented_images/Lars\"\n",
    "MORGAN = BASE_PATH + \"augmented_images/Morgan\"\n",
    "OTHER = BASE_PATH + \"augmented_images/Other\"\n",
    "\n",
    "LABELS = {LARS: 0, MORGAN: 1, OTHER: 2, KJARTAN: 3}\n",
    "\n",
    "image_paths = []\n",
    "for label in LABELS:\n",
    "    count = 0\n",
    "    for image_file in tqdm(os.listdir(label)):\n",
    "        if not image_file.endswith(\".jpg\"):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(label, image_file)\n",
    "        image_paths.append((image_path, label))\n",
    "\n",
    "        \n",
    "np.random.shuffle(image_paths)\n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for image_path, label in tqdm(image_paths):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    h,w = image.shape\n",
    "    image = np.reshape(image, (1, h, w))\n",
    "    training_data.append(image)\n",
    "    training_labels.append(LABELS[label])\n",
    "    \n",
    "\n",
    "save_file_name = os.path.join(BASE_PATH, f\"lars_morgan_other_augments_{IMAGE_SIZE}px_{len(training_data)}.torch\")\n",
    "\n",
    "print(\"Saving torch object to file\")\n",
    "torch.save(\n",
    "    {\n",
    "        #\"x\": torch.Tensor(training_data).view(-1, IMAGE_SIZE, IMAGE_SIZE) / 255.0,\n",
    "        \"x\": torch.Tensor(training_data) / 255.0,\n",
    "        \"y\": torch.Tensor(training_labels).to(torch.int64),\n",
    "        \"num_classes\": 3,\n",
    "    },\n",
    "    save_file_name,\n",
    ")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cloud_directory = \"faces/tensors/\" + f\"lars_morgan_other_augments_{IMAGE_SIZE}px_{len(training_data)}.torch\"\n",
    "blob = bucket.blob(cloud_directory)\n",
    "\n",
    "source_file_name = save_file_name\n",
    "blob.upload_from_filename(source_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Currently every single frame of the video is used, so each frame needs to contain the face of the person\n",
    "- All videos should use the same camera settings (resolution etc)\n",
    "- Should film with as low resolution as possible\n",
    "- Low framerate is probably ideal\n",
    "- When saving videos, store them as `folder/Class/video.mp4` and zip `folder`"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
